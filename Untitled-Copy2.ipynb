{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_document(doco):\n",
    "    punctuation = string.punctuation + '\\n\\n'\n",
    "    punc_replace = ''.join([' ' for s in punctuation])\n",
    "    doco_clean = doco.replace('-', ' ')\n",
    "    doco_alphas = re.sub(r'\\W +', '', doco_clean)\n",
    "    trans_table = str.maketrans(punctuation, punc_replace)\n",
    "    doco_clean = ' '.join([word.translate(trans_table) for word in doco_alphas.split(' ')])\n",
    "    doco_clean = doco_clean.split(' ')\n",
    "    doco_clean = [word.lower() for word in doco_clean if len(word) > 0]\n",
    "    \n",
    "    return doco_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_creator(file, inp_len):\n",
    "    X = []\n",
    "    label = []\n",
    "    for line in file:\n",
    "        cline = clean_document(line)\n",
    "        length = len(cline)\n",
    "        if length <= inp_len:\n",
    "            continue\n",
    "        for i in range(0, length - inp_len):\n",
    "            X.append(cline[i:i+inp_len])\n",
    "            label.append(cline[i+inp_len])\n",
    "        X.append(cline[i+1:])\n",
    "        label.append('<EOS>')\n",
    "    return X, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_train = open('train.txt')\n",
    "file_test = open('test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_temp, y_train_temp = dataset_creator(file_train, timesteps)\n",
    "X_test, y_test = dataset_creator(file_test, timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3440 18089\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for el in y_train_temp:\n",
    "    if el == '<EOS>':\n",
    "        i += 1\n",
    "        \n",
    "print(i, len(y_train_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos = 0\n",
    "the = 0\n",
    "to = 0\n",
    "andc = 0\n",
    "a = 0\n",
    "of = 0\n",
    "inc = 0\n",
    "forc = 0\n",
    "you = 0\n",
    "isc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.random.choice(len(y_train_temp), size = len(y_train_temp), replace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = []\n",
    "X_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ind:\n",
    "    if y_train_temp[i] == '<EOS>':\n",
    "        eos += 1\n",
    "        if eos <= 200:\n",
    "            y_train.append(y_train_temp[i])\n",
    "            X_train.append(X_train_temp[i])\n",
    "            \n",
    "    elif y_train_temp[i] == 'the':\n",
    "        the += 1\n",
    "        if the <= 200:\n",
    "            y_train.append(y_train_temp[i])\n",
    "            X_train.append(X_train_temp[i])\n",
    "            \n",
    "    elif y_train_temp[i] == 'to':\n",
    "        to += 1\n",
    "        if to <= 200:\n",
    "            y_train.append(y_train_temp[i])\n",
    "            X_train.append(X_train_temp[i])\n",
    "            \n",
    "    elif y_train_temp[i] == 'and':\n",
    "        andc += 1\n",
    "        if andc <= 200:\n",
    "            y_train.append(y_train_temp[i])\n",
    "            X_train.append(X_train_temp[i])\n",
    "            \n",
    "    elif y_train_temp[i] == 'a':\n",
    "        a += 1\n",
    "        if a <= 200:\n",
    "            y_train.append(y_train_temp[i])\n",
    "            X_train.append(X_train_temp[i])\n",
    "            \n",
    "    elif y_train_temp[i] == 'of':\n",
    "        of += 1\n",
    "        if of <= 200:\n",
    "            y_train.append(y_train_temp[i])\n",
    "            X_train.append(X_train_temp[i])\n",
    "            \n",
    "    elif y_train_temp[i] == 'in':\n",
    "        inc += 1\n",
    "        if inc <= 200:\n",
    "            y_train.append(y_train_temp[i])\n",
    "            X_train.append(X_train_temp[i])\n",
    "            \n",
    "    elif y_train_temp[i] == 'for':\n",
    "        forc += 1\n",
    "        if forc <= 200:\n",
    "            y_train.append(y_train_temp[i])\n",
    "            X_train.append(X_train_temp[i])\n",
    "            \n",
    "    elif y_train_temp[i] == 'you':\n",
    "        you += 1\n",
    "        if you <= 200:\n",
    "            y_train.append(y_train_temp[i])\n",
    "            X_train.append(X_train_temp[i])\n",
    "            \n",
    "    elif y_train_temp[i] == 'is':\n",
    "        isc += 1\n",
    "        if isc <= 200:\n",
    "            y_train.append(y_train_temp[i])\n",
    "            X_train.append(X_train_temp[i])\n",
    "            \n",
    "    else:\n",
    "        y_train.append(y_train_temp[i])\n",
    "        X_train.append(X_train_temp[i])\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13879"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "i = 0\n",
    "for row in X_train_temp:\n",
    "    for el in row:\n",
    "        if el not in vocab:\n",
    "            vocab[el] = i\n",
    "            i += 1\n",
    "            \n",
    "for row in X_test:\n",
    "    for el in row:\n",
    "        if el not in vocab:\n",
    "            vocab[el] = i\n",
    "            i += 1\n",
    "vocab['<EOS>'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6498"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_vocab = {}\n",
    "for key in vocab.keys():\n",
    "    reverse_vocab[vocab[key]] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_creator(X,y, vocab = vocab, input_len = timesteps):\n",
    "    vocab_len = len(vocab)\n",
    "    one_hotX = []\n",
    "    one_hotY = []\n",
    "    for row in X:\n",
    "        temp = np.zeros(shape = (input_len, vocab_len))\n",
    "        for (i,el) in enumerate(row):\n",
    "            temp[i][vocab[el]] = 1\n",
    "        one_hotX.append(temp)\n",
    "            \n",
    "    for row in y:\n",
    "        temp = np.zeros(shape = (vocab_len,))\n",
    "        temp[vocab[row]] = 1\n",
    "        one_hotY.append(temp)\n",
    "        \n",
    "    return np.array(one_hotX), np.array(one_hotY)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_oh, y_train_oh = one_hot_creator(X_train[:128], y_train[:128])   #one_hot vectors\n",
    "# # X_test_oh, y_test_oh = one_hot_creator(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = 0\n",
    "for i in y_train_oh[0]:\n",
    "    if i == 1:\n",
    "        z += 1\n",
    "        \n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'the', 'insidious', 'conspiracy']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-d68eea4137f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 't' is not defined"
     ]
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.ones((3,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = np.ones((1000,5))\n",
    "f = np.ones(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1001., 1001., 1001., 1001., 1001.],\n",
       "       [1001., 1001., 1001., 1001., 1001.],\n",
       "       [1001., 1001., 1001., 1001., 1001.]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = np.dot(d,e) + f\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.31590559, 1.21920605],\n",
       "       [2.31590559, 1.21920605],\n",
       "       [2.31590559, 1.21920605]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.ones((3,2))\n",
    "b = np.ones((3,2))\n",
    "c = np.random.uniform((3,2))\n",
    "\n",
    "z = np.multiply(a,b)\n",
    "np.multiply(z,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU():\n",
    "    def __init__(self,hidden_units, embed_len, batch_size, timesteps):\n",
    "        \n",
    "        self.hidden_units = hidden_units\n",
    "        self.Wc = np.random.normal(size = (embed_len + self.hidden_units, self.hidden_units)) \n",
    "        self.Wu = np.random.normal(size = (embed_len + self.hidden_units, self.hidden_units)) \n",
    "        self.bc = np.random.normal(size = (1, self.hidden_units)) \n",
    "        self.bu = np.random.normal(size = (1, self.hidden_units))         \n",
    "        self.batch_size = batch_size\n",
    "        self.clist = []\n",
    "        self.glist = []\n",
    "        self.tlist = []\n",
    "        self.c_initial = np.zeros(shape = (self.batch_size, self.hidden_units))\n",
    "        self.timesteps = timesteps\n",
    "        \n",
    "    def forward(self,X):\n",
    "        c = self.c_initial\n",
    "        for i in range(self.timesteps):\n",
    "            conc_inp = np.concatenate((X[:,i,:], c), axis = 1)\n",
    "            \n",
    "            tilda_inp = np.dot(conc_inp, self.Wc) + self.bc\n",
    "            c_tilda = self.tanh(tilda_inp)\n",
    "            self.tlist.append(tilda_inp)\n",
    "            \n",
    "            gamma_inp = np.dot(conc_inp, self.Wu) + self.bu\n",
    "            gammau = self.sigmoid(gamma_inp)\n",
    "            self.glist.append(gamma_inp)\n",
    "            \n",
    "            c = np.multiply(gammau, c_tilda) + np.multiply(1-gammau, c)\n",
    "            self.clist.append(c)\n",
    "        \n",
    "        return c  \n",
    "    \n",
    "    def backward(self, prev_dev, X, lr, gmin, gmax):\n",
    "        \n",
    "        for i in range(self.timesteps):\n",
    "            ind = self.timesteps - 1 - i\n",
    "            if ind > 0:\n",
    "                conc_inp = np.concatenate((X[:,ind,:], self.clist[ind-1]), axis = 1)\n",
    "            else:\n",
    "                conc_inp = np.concatenate((X[:,ind,:], self.c_initial), axis = 1)\n",
    "            \n",
    "            gammau = self.sigmoid(self.glist[ind])\n",
    "            \n",
    "            tanderv = 1 - (self.tanh(self.tlist[ind]))**2\n",
    "            sigmaderv = gammau*(1-gammau)\n",
    "            \n",
    "            c_tilda = self.tanh(self.tlist[ind])\n",
    "            inp_transpose = np.transpose(conc_inp)\n",
    "            \n",
    "            temp = np.multiply(gammau, tanderv)\n",
    "            temp = np.multiply(temp, prev_dev)\n",
    "            \n",
    "            gradWc = np.clip(np.dot(inp_transpose, temp)/self.batch_size, gmin, gmax)\n",
    "            gradbc = np.clip(np.sum(temp, axis = 0)/self.batch_size, gmin, gmax)\n",
    "            \n",
    "            if ind > 0:\n",
    "                temp = np.multiply(c_tilda - self.clist[ind - 1], sigmaderv)\n",
    "            else:\n",
    "                temp = np.multiply(c_tilda - self.c_initial, sigmaderv)\n",
    "            \n",
    "            temp = np.multiply(temp, prev_dev) \n",
    "            \n",
    "            gradWu = np.clip(np.dot(inp_transpose, temp)/self.batch_size, gmin, gmax)\n",
    "            gradbu = np.clip(np.sum(temp, axis = 0)/self.batch_size, gmin, gmax)\n",
    "            \n",
    "            self.Wc -= lr*gradWc\n",
    "            self.bc -= lr*gradbc\n",
    "            self.Wu -= lr*gradWu\n",
    "            self.bu -= lr*gradbu\n",
    "        \n",
    "        self.clist = []\n",
    "        self.tlist = []\n",
    "        self.glist = []\n",
    "        \n",
    "    def sigmoid(self, X):\n",
    "        return ( 1/ (1 + np.exp(-X)))\n",
    "    \n",
    "    def tanh(self, X):\n",
    "        p = np.exp(X)\n",
    "        m = np.exp(-X)\n",
    "        return((p-m)/(p+m))  \n",
    "    \n",
    "    def load_param(self, param):\n",
    "        self.Wc = param['Wc']\n",
    "        self.bc = param['bc']\n",
    "        self.Wu = param['Wu']\n",
    "        self.bu = param['bu']\n",
    "        \n",
    "    def save_param(self, param):\n",
    "        param['Wc'] = self.Wc\n",
    "        param['bc'] = self.bc\n",
    "        param['Wu'] = self.Wu\n",
    "        param['bu'] = self.bu\n",
    "        return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network():\n",
    "    def __init__(self, hidden_units, embed_len, output_size, batch_size, timesteps):\n",
    "        self.gru = GRU(hidden_units, embed_len, batch_size, timesteps)\n",
    "        self.W = np.random.normal(size = (hidden_units, output_size))\n",
    "        self.b = np.random.normal(size = (1, output_size))\n",
    "        self.batch_size = batch_size\n",
    "        self.c = 0\n",
    "        self.o = 0\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.c = self.gru.forward(X)\n",
    "        self.o = self.softmax(np.dot(self.c, self.W) + self.b)\n",
    "        return self.o\n",
    "    \n",
    "    def backward(self, X, y, lr, gmin, gmax):\n",
    "        grad = self.o - y\n",
    "\n",
    "        gradW = np.clip(np.dot(np.transpose(self.c), grad)/self.batch_size, gmin, gmax)\n",
    "        gradb = np.clip(np.sum(grad, axis = 0)/self.batch_size, gmin, gmax)\n",
    "\n",
    "        self.c = 0\n",
    "        self.o = 0\n",
    "                        \n",
    "        grad_to_backprop = np.dot(grad, np.transpose(self.W))\n",
    "        \n",
    "        self.W -= lr*gradW\n",
    "        self.b -= lr*gradb\n",
    "        \n",
    "        self.gru.backward(grad_to_backprop, X, lr, gmin, gmax)\n",
    "        \n",
    "    def softmax(self, X):\n",
    "        exps = np.exp(X - np.reshape(np.max(X, axis = 1), (X.shape[0], 1)))\n",
    "        return exps / np.reshape(np.sum(exps, axis = 1), (X.shape[0], 1))\n",
    "    \n",
    "    def load_param(self, param):\n",
    "        self.W = param['W']\n",
    "        self.b = param['b']\n",
    "        self.gru.load_param(param)\n",
    "        \n",
    "    def save_param(self):\n",
    "        param = {}\n",
    "        param['W'] = self.W\n",
    "        param['b'] = self.b\n",
    "        return(self.gru.save_param(param))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_calc(pred, actual):\n",
    "    mult = np.multiply(np.log(pred), actual)\n",
    "    return -np.sum(mult)/pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, net, epochs, lr, loss_list, batch_size, cont_from = 0, model_name = 'model', gmin = -10, gmax = 10, time_to_save = 1):\n",
    "    train_len = len(X)\n",
    "    for i in range(1, epochs+1):\n",
    "        tic = time.time()\n",
    "        counter = 0\n",
    "        \n",
    "        for j in range(0, train_len, batch_size):\n",
    "            \n",
    "            if(j+batch_size > train_len):\n",
    "                X_batch, y_batch = one_hot_creator(X[train_len-batch_size:], y[train_len-batch_size:])\n",
    "            else:\n",
    "                X_batch, y_batch = one_hot_creator(X[j:j+batch_size], y[j:j+batch_size])\n",
    "                \n",
    "            pred = net.forward(X_batch)\n",
    "            loss = loss_calc(pred, y_batch)\n",
    "            \n",
    "            loss_list.append(loss)\n",
    "            net.backward(X_batch, y_batch, lr, gmin, gmax)\n",
    "            counter += 1\n",
    "            \n",
    "        if (cont_from + i) % time_to_save == 0:\n",
    "            param_dict = net.save_param()\n",
    "            with open('param_epoch_' + model_name + \"_\" + str(cont_from + i) + '.pkl', 'wb') as f:\n",
    "                pickle.dump(param_dict, f)\n",
    "        \n",
    "        ep_time = time.time() - tic\n",
    "        print(\"Epoch: %d --> Average Loss: %.3f completed in %.3f seconds\" \n",
    "              %(cont_from + i, sum(loss_list[len(loss_list)-counter:]) / counter, ep_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_new = Network(256, len(vocab), len(vocab), 128, timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 --> Average Loss: 31.504 completed in 87.400 seconds\n",
      "Epoch: 2 --> Average Loss: 10.439 completed in 86.390 seconds\n",
      "Epoch: 3 --> Average Loss: 8.507 completed in 99.886 seconds\n",
      "Epoch: 4 --> Average Loss: 8.147 completed in 87.040 seconds\n",
      "Epoch: 5 --> Average Loss: 7.971 completed in 85.447 seconds\n",
      "Epoch: 6 --> Average Loss: 7.855 completed in 85.008 seconds\n",
      "Epoch: 7 --> Average Loss: 7.768 completed in 84.629 seconds\n",
      "Epoch: 8 --> Average Loss: 7.699 completed in 84.792 seconds\n",
      "Epoch: 9 --> Average Loss: 7.641 completed in 82.655 seconds\n",
      "Epoch: 10 --> Average Loss: 7.591 completed in 83.798 seconds\n",
      "Epoch: 11 --> Average Loss: 7.547 completed in 85.147 seconds\n",
      "Epoch: 12 --> Average Loss: 7.507 completed in 86.720 seconds\n",
      "Epoch: 13 --> Average Loss: 7.471 completed in 81.918 seconds\n",
      "Epoch: 14 --> Average Loss: 7.438 completed in 83.135 seconds\n",
      "Epoch: 15 --> Average Loss: 7.407 completed in 87.492 seconds\n"
     ]
    }
   ],
   "source": [
    "loss_list_new = []\n",
    "train(X_train, y_train, net_new, 15, 1, loss_list_new, 128, model_name='net_new', time_to_save=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_new.gru.clist = []\n",
    "net_new.gru.tlist = []\n",
    "net_new.gru.glist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 --> Average Loss: 7.392 completed in 102.716 seconds\n",
      "Epoch: 17 --> Average Loss: 7.371 completed in 101.886 seconds\n",
      "Epoch: 18 --> Average Loss: 7.356 completed in 95.714 seconds\n",
      "Epoch: 19 --> Average Loss: 7.342 completed in 93.644 seconds\n",
      "Epoch: 20 --> Average Loss: 7.329 completed in 94.560 seconds\n"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, net_new, 5, 0.5, loss_list_new, 128, cont_from= 15, model_name='net_new5', time_to_save=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 --> Average Loss: 7.303 completed in 99.440 seconds\n",
      "Epoch: 22 --> Average Loss: 7.286 completed in 97.793 seconds\n",
      "Epoch: 23 --> Average Loss: 7.263 completed in 93.564 seconds\n",
      "Epoch: 24 --> Average Loss: 7.240 completed in 100.380 seconds\n",
      "Epoch: 25 --> Average Loss: 7.218 completed in 91.209 seconds\n"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, net_new, 5, 1, loss_list_new, 128, cont_from= 20, model_name='net_new5', time_to_save=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 --> Average Loss: 7.196 completed in 88.926 seconds\n"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, net_new, 1, 2, loss_list_new, 128, cont_from= 25, model_name='net_new', time_to_save=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 --> Average Loss: 7.168 completed in 84.341 seconds\n",
      "Epoch: 28 --> Average Loss: 7.132 completed in 96.998 seconds\n",
      "Epoch: 29 --> Average Loss: 7.098 completed in 106.015 seconds\n",
      "Epoch: 30 --> Average Loss: 7.065 completed in 107.353 seconds\n",
      "Epoch: 31 --> Average Loss: 7.032 completed in 81.742 seconds\n"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, net_new, 5, 2, loss_list_new, 128, cont_from= 26, model_name='net_new', time_to_save=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('param_epoch_net_new_30.pkl', 'rb') as f:\n",
    "    tada = pickle.load(f)\n",
    "net_new.load_param(tada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_fasttext = open('fasttext_data.txt', \"w\")\n",
    "count = 0\n",
    "for key in vocab.keys():\n",
    "    file_fasttext.write(key)\n",
    "    count += 1\n",
    "    if count == 10:\n",
    "        file_fasttext.write(\"\\n\")\n",
    "        count = 0\n",
    "        continue\n",
    "    file_fasttext.write(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.skipgram('fasttext_data.txt', 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.load_model('model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_ft = {}\n",
    "\n",
    "for row in X_train_temp:\n",
    "    for el in row:\n",
    "        if el not in vocab_ft:\n",
    "            vocab_ft[el] = model[el]\n",
    "            i += 1\n",
    "            \n",
    "for row in X_test:\n",
    "    for el in row:\n",
    "        if el not in vocab_ft:\n",
    "            vocab_ft[el] = model[el]\n",
    "            i += 1\n",
    "vocab_ft['<EOS>'] = model['<EOS>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6683"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len = len(vocab)\n",
    "    one_hotX = []\n",
    "    one_hotY = []\n",
    "    for row in X:\n",
    "        temp = np.zeros(shape = (input_len, vocab_len))\n",
    "        for (i,el) in enumerate(row):\n",
    "            temp[i][vocab[el]] = 1\n",
    "        one_hotX.append(temp)\n",
    "            \n",
    "    \n",
    "        \n",
    "    return np.array(one_hotX), np.array(one_hotY)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ft_embed(X,y, vocab = vocab_ft, vocab2 = vocab):\n",
    "    vocab_len = len(vocab)\n",
    "    ft_embedX = []\n",
    "    one_hotY = []\n",
    "    for row in X:\n",
    "        temp = []\n",
    "        for el in row:\n",
    "            temp.append(vocab[el])\n",
    "        ft_embedX.append(np.array(temp))\n",
    "        \n",
    "    for row in y:\n",
    "        temp = np.zeros(shape = (vocab_len,))\n",
    "        temp[vocab2[row]] = 1\n",
    "        one_hotY.append(temp)\n",
    "        \n",
    "    return np.array(ft_embedX), np.array(one_hotY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx, ty = ft_embed(X_train[:128], y_train[:128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6683,)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ty[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train2(X, y, net, epochs, lr, loss_list, batch_size, cont_from = 0, model_name = 'model', gmin = -10, gmax = 10, time_to_save = 1):\n",
    "    train_len = len(X)\n",
    "    for i in range(1, epochs+1):\n",
    "        tic = time.time()\n",
    "        counter = 0\n",
    "        \n",
    "        for j in range(0, train_len, batch_size):\n",
    "            \n",
    "            if(j+batch_size > train_len):\n",
    "                X_batch, y_batch = ft_embed(X[train_len-batch_size:], y[train_len-batch_size:])\n",
    "            else:\n",
    "                X_batch, y_batch = ft_embed(X[j:j+batch_size], y[j:j+batch_size])\n",
    "                \n",
    "            pred = net.forward(X_batch)\n",
    "            loss = loss_calc(pred, y_batch)\n",
    "            \n",
    "            loss_list.append(loss)\n",
    "            net.backward(X_batch, y_batch, lr, gmin, gmax)\n",
    "            counter += 1\n",
    "            \n",
    "        if (cont_from + i) % time_to_save == 0:\n",
    "            param_dict = net.save_param()\n",
    "            with open('param_epoch_' + model_name + \"_\" + str(cont_from + i) + '.pkl', 'wb') as f:\n",
    "                pickle.dump(param_dict, f)\n",
    "        \n",
    "        ep_time = time.time() - tic\n",
    "        print(\"Epoch: %d --> Average Loss: %.3f completed in %.3f seconds\" \n",
    "              %(cont_from + i, sum(loss_list[len(loss_list)-counter:]) / counter, ep_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_ft = Network(256, 100, len(vocab), 128, timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 --> Average Loss: 24.561 completed in 35.432 seconds\n",
      "Epoch: 2 --> Average Loss: 15.106 completed in 35.501 seconds\n",
      "Epoch: 3 --> Average Loss: 11.789 completed in 37.275 seconds\n",
      "Epoch: 4 --> Average Loss: 10.330 completed in 36.917 seconds\n",
      "Epoch: 5 --> Average Loss: 9.850 completed in 33.803 seconds\n",
      "Epoch: 6 --> Average Loss: 9.726 completed in 33.815 seconds\n",
      "Epoch: 7 --> Average Loss: 9.961 completed in 35.073 seconds\n",
      "Epoch: 8 --> Average Loss: 9.299 completed in 34.299 seconds\n",
      "Epoch: 9 --> Average Loss: 9.382 completed in 35.244 seconds\n",
      "Epoch: 10 --> Average Loss: 9.292 completed in 33.752 seconds\n",
      "Epoch: 11 --> Average Loss: 9.138 completed in 33.757 seconds\n",
      "Epoch: 12 --> Average Loss: 8.854 completed in 33.749 seconds\n",
      "Epoch: 13 --> Average Loss: 9.716 completed in 33.818 seconds\n",
      "Epoch: 14 --> Average Loss: 8.960 completed in 34.117 seconds\n",
      "Epoch: 15 --> Average Loss: 9.703 completed in 33.976 seconds\n"
     ]
    }
   ],
   "source": [
    "loss_list_ft = []\n",
    "train2(X_train, y_train, net_ft, 15, 1, loss_list_ft, 128, model_name='net_ft', time_to_save=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 --> Average Loss: 8.909 completed in 34.211 seconds\n",
      "Epoch: 17 --> Average Loss: 8.639 completed in 35.102 seconds\n",
      "Epoch: 18 --> Average Loss: 8.832 completed in 34.498 seconds\n",
      "Epoch: 19 --> Average Loss: 8.806 completed in 33.328 seconds\n",
      "Epoch: 20 --> Average Loss: 8.579 completed in 33.838 seconds\n",
      "Epoch: 21 --> Average Loss: 8.641 completed in 33.988 seconds\n",
      "Epoch: 22 --> Average Loss: 8.534 completed in 34.277 seconds\n",
      "Epoch: 23 --> Average Loss: 8.900 completed in 33.853 seconds\n",
      "Epoch: 24 --> Average Loss: 8.840 completed in 34.239 seconds\n",
      "Epoch: 25 --> Average Loss: 8.497 completed in 34.561 seconds\n"
     ]
    }
   ],
   "source": [
    "train2(X_train, y_train, net_ft, 10, 1, loss_list_ft, 128, cont_from = 15, model_name='net_ft', time_to_save=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 --> Average Loss: 7.624 completed in 34.956 seconds\n",
      "Epoch: 27 --> Average Loss: 7.531 completed in 34.186 seconds\n",
      "Epoch: 28 --> Average Loss: 7.434 completed in 33.629 seconds\n",
      "Epoch: 29 --> Average Loss: 7.525 completed in 33.307 seconds\n",
      "Epoch: 30 --> Average Loss: 7.442 completed in 33.802 seconds\n",
      "Epoch: 31 --> Average Loss: 7.521 completed in 33.994 seconds\n",
      "Epoch: 32 --> Average Loss: 7.437 completed in 33.353 seconds\n",
      "Epoch: 33 --> Average Loss: 7.518 completed in 34.213 seconds\n",
      "Epoch: 34 --> Average Loss: 7.429 completed in 36.682 seconds\n",
      "Epoch: 35 --> Average Loss: 7.516 completed in 34.633 seconds\n"
     ]
    }
   ],
   "source": [
    "train2(X_train, y_train, net_ft, 10, 0.5, loss_list_ft, 128, cont_from = 25, model_name='net_ft', time_to_save=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('param_epoch_net_ft_35.pkl', 'rb') as f:\n",
    "    tada = pickle.load(f)\n",
    "net_ft.load_param(tada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 --> Average Loss: 7.413 completed in 31.832 seconds\n",
      "Epoch: 37 --> Average Loss: 7.490 completed in 32.313 seconds\n",
      "Epoch: 38 --> Average Loss: 7.478 completed in 31.427 seconds\n",
      "Epoch: 39 --> Average Loss: 7.511 completed in 33.625 seconds\n",
      "Epoch: 40 --> Average Loss: 7.413 completed in 33.131 seconds\n"
     ]
    }
   ],
   "source": [
    "train2(X_train, y_train, net_ft, 5, 0.5, loss_list_ft, 128, cont_from = 35, model_name='net_ft', time_to_save=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41 --> Average Loss: 7.502 completed in 41.957 seconds\n",
      "Epoch: 42 --> Average Loss: 7.432 completed in 36.503 seconds\n",
      "Epoch: 43 --> Average Loss: 7.534 completed in 36.062 seconds\n",
      "Epoch: 44 --> Average Loss: 7.471 completed in 37.717 seconds\n",
      "Epoch: 45 --> Average Loss: 7.507 completed in 33.013 seconds\n",
      "Epoch: 46 --> Average Loss: 7.389 completed in 32.726 seconds\n",
      "Epoch: 47 --> Average Loss: 7.474 completed in 32.748 seconds\n",
      "Epoch: 48 --> Average Loss: 7.505 completed in 31.120 seconds\n",
      "Epoch: 49 --> Average Loss: 7.386 completed in 32.400 seconds\n",
      "Epoch: 50 --> Average Loss: 7.415 completed in 36.221 seconds\n"
     ]
    }
   ],
   "source": [
    "train2(X_train, y_train, net_ft, 10, 0.5, loss_list_ft, 128, cont_from = 40, model_name='net_ft', time_to_save=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51 --> Average Loss: 7.262 completed in 35.078 seconds\n",
      "Epoch: 52 --> Average Loss: 7.219 completed in 33.299 seconds\n",
      "Epoch: 53 --> Average Loss: 7.217 completed in 39.256 seconds\n",
      "Epoch: 54 --> Average Loss: 7.216 completed in 43.801 seconds\n",
      "Epoch: 55 --> Average Loss: 7.215 completed in 41.157 seconds\n"
     ]
    }
   ],
   "source": [
    "train2(X_train, y_train, net_ft, 5, 0.1, loss_list_ft, 128, cont_from = 50, model_name='net_ft', time_to_save=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56 --> Average Loss: 7.310 completed in 43.103 seconds\n",
      "Epoch: 57 --> Average Loss: 7.256 completed in 44.069 seconds\n",
      "Epoch: 58 --> Average Loss: 7.238 completed in 41.757 seconds\n",
      "Epoch: 59 --> Average Loss: 7.233 completed in 40.973 seconds\n",
      "Epoch: 60 --> Average Loss: 7.232 completed in 40.473 seconds\n"
     ]
    }
   ],
   "source": [
    "train2(X_train, y_train, net_ft, 5, 0.01, loss_list_ft, 128, cont_from = 55, model_name='net_ft', time_to_save=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_oh, y_train_oh = one_hot_creator(X_train[400:528], y_train[400:528])   #one_hot vectors\n",
    "#X_train_oh, y_train_oh = ft_embed(X_train[400:528], y_train[400:528])   #one_hot vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = net_new.forward(X_train_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max(arr, window):\n",
    "    return np.argsort(arr)[-window:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 3])"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([21,1,3,25,7])\n",
    "find_max(a, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 5\n",
    "l = []\n",
    "for el in p:\n",
    "    l.append(find_max(el, window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "yint = np.argmax(y_train_oh, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yint[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0\n",
    "for i,el in enumerate(l):\n",
    "    if yint[i] in el:\n",
    "        acc += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.078125"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 6683)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network(256, len(vocab), len(vocab), 128, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('param_epoch_net2 91.pkl', 'rb') as f:\n",
    "    tada = pickle.load(f)\n",
    "net2.load_param(tada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 --> Average Loss: 56.538 completed in 110.000 seconds\n",
      "Epoch: 2 --> Average Loss: 54.516 completed in 110.083 seconds\n",
      "Epoch: 3 --> Average Loss: 53.196 completed in 110.757 seconds\n",
      "Epoch: 4 --> Average Loss: 52.092 completed in 111.730 seconds\n",
      "Epoch: 5 --> Average Loss: 51.070 completed in 108.668 seconds\n",
      "Epoch: 6 --> Average Loss: 50.199 completed in 110.120 seconds\n",
      "Epoch: 7 --> Average Loss: 49.365 completed in 111.043 seconds\n",
      "Epoch: 8 --> Average Loss: 48.562 completed in 110.859 seconds\n",
      "Epoch: 9 --> Average Loss: 47.839 completed in 111.537 seconds\n",
      "Epoch: 10 --> Average Loss: 47.220 completed in 108.511 seconds\n",
      "Epoch: 11 --> Average Loss: 46.601 completed in 110.023 seconds\n",
      "Epoch: 12 --> Average Loss: 45.974 completed in 111.316 seconds\n",
      "Epoch: 13 --> Average Loss: 45.424 completed in 109.929 seconds\n",
      "Epoch: 14 --> Average Loss: 44.875 completed in 115.560 seconds\n",
      "Epoch: 15 --> Average Loss: 44.303 completed in 111.134 seconds\n",
      "Epoch: 16 --> Average Loss: 43.813 completed in 110.129 seconds\n",
      "Epoch: 17 --> Average Loss: 43.341 completed in 109.214 seconds\n",
      "Epoch: 18 --> Average Loss: 42.902 completed in 110.003 seconds\n",
      "Epoch: 19 --> Average Loss: 42.526 completed in 111.369 seconds\n",
      "Epoch: 20 --> Average Loss: 42.146 completed in 111.001 seconds\n",
      "Epoch: 21 --> Average Loss: 41.765 completed in 111.149 seconds\n",
      "Epoch: 22 --> Average Loss: 41.424 completed in 110.096 seconds\n",
      "Epoch: 23 --> Average Loss: 41.111 completed in 110.398 seconds\n",
      "Epoch: 24 --> Average Loss: 40.843 completed in 110.522 seconds\n",
      "Epoch: 25 --> Average Loss: 40.572 completed in 109.608 seconds\n",
      "Epoch: 26 --> Average Loss: 40.287 completed in 109.904 seconds\n",
      "Epoch: 27 --> Average Loss: 40.028 completed in 112.674 seconds\n",
      "Epoch: 28 --> Average Loss: 39.790 completed in 112.054 seconds\n",
      "Epoch: 29 --> Average Loss: 39.559 completed in 118.679 seconds\n",
      "Epoch: 30 --> Average Loss: 39.324 completed in 110.005 seconds\n",
      "Epoch: 31 --> Average Loss: 39.113 completed in 110.754 seconds\n",
      "Epoch: 32 --> Average Loss: 38.891 completed in 111.651 seconds\n",
      "Epoch: 33 --> Average Loss: 38.671 completed in 109.227 seconds\n",
      "Epoch: 34 --> Average Loss: 38.480 completed in 112.102 seconds\n",
      "Epoch: 35 --> Average Loss: 38.265 completed in 111.180 seconds\n",
      "Epoch: 36 --> Average Loss: 38.051 completed in 111.897 seconds\n",
      "Epoch: 37 --> Average Loss: 37.846 completed in 109.779 seconds\n",
      "Epoch: 38 --> Average Loss: 37.641 completed in 111.003 seconds\n",
      "Epoch: 39 --> Average Loss: 37.409 completed in 111.447 seconds\n",
      "Epoch: 40 --> Average Loss: 37.194 completed in 111.449 seconds\n",
      "Epoch: 41 --> Average Loss: 37.001 completed in 111.684 seconds\n",
      "Epoch: 42 --> Average Loss: 36.798 completed in 111.593 seconds\n",
      "Epoch: 43 --> Average Loss: 36.612 completed in 111.496 seconds\n",
      "Epoch: 44 --> Average Loss: 36.451 completed in 110.967 seconds\n",
      "Epoch: 45 --> Average Loss: 36.312 completed in 111.822 seconds\n",
      "Epoch: 46 --> Average Loss: 36.174 completed in 105.519 seconds\n",
      "Epoch: 47 --> Average Loss: 36.011 completed in 99.454 seconds\n",
      "Epoch: 48 --> Average Loss: 35.820 completed in 99.415 seconds\n",
      "Epoch: 49 --> Average Loss: 35.618 completed in 100.442 seconds\n",
      "Epoch: 50 --> Average Loss: 35.438 completed in 100.310 seconds\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "train(X_train, y_train, net, 50, 0.01, loss_list, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51 --> Average Loss: 35.246 completed in 104.009 seconds\n",
      "Epoch: 52 --> Average Loss: 35.230 completed in 105.677 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-620dbd1d1fe8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcont_from\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-110-860fbed2d9fa>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(X, y, net, epochs, lr, loss_list, batch_size, gmin, gmax, time_to_save, cont_from)\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_calc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-f643dd352f90>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-f643dd352f90>\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mexps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mexps\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, net, 50, 0.001, loss_list, 128, cont_from=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('loss_net1', np.array(loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('loss_net2', np.array(loss_list2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = Network(1024, len(vocab), len(vocab), 256, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 --> Average Loss: 114.529 completed in 280.236 seconds\n",
      "Epoch: 2 --> Average Loss: 111.436 completed in 278.928 seconds\n",
      "Epoch: 3 --> Average Loss: 108.830 completed in 279.059 seconds\n",
      "Epoch: 4 --> Average Loss: 106.986 completed in 278.852 seconds\n",
      "Epoch: 5 --> Average Loss: 105.232 completed in 280.288 seconds\n",
      "Epoch: 6 --> Average Loss: 103.848 completed in 278.417 seconds\n",
      "Epoch: 7 --> Average Loss: 102.589 completed in 279.409 seconds\n",
      "Epoch: 8 --> Average Loss: 101.249 completed in 279.429 seconds\n",
      "Epoch: 9 --> Average Loss: 100.077 completed in 277.888 seconds\n",
      "Epoch: 10 --> Average Loss: 99.031 completed in 277.133 seconds\n",
      "Epoch: 11 --> Average Loss: 97.971 completed in 278.471 seconds\n",
      "Epoch: 12 --> Average Loss: 96.830 completed in 256.688 seconds\n",
      "Epoch: 13 --> Average Loss: 95.764 completed in 249.403 seconds\n",
      "Epoch: 14 --> Average Loss: 94.895 completed in 250.579 seconds\n",
      "Epoch: 15 --> Average Loss: 94.046 completed in 252.558 seconds\n",
      "Epoch: 16 --> Average Loss: 93.177 completed in 248.827 seconds\n",
      "Epoch: 17 --> Average Loss: 92.318 completed in 249.751 seconds\n",
      "Epoch: 18 --> Average Loss: 91.394 completed in 250.058 seconds\n",
      "Epoch: 19 --> Average Loss: 90.630 completed in 249.594 seconds\n",
      "Epoch: 20 --> Average Loss: 89.892 completed in 249.921 seconds\n",
      "Epoch: 21 --> Average Loss: 89.278 completed in 249.828 seconds\n",
      "Epoch: 22 --> Average Loss: 88.601 completed in 249.257 seconds\n",
      "Epoch: 23 --> Average Loss: 87.937 completed in 249.519 seconds\n",
      "Epoch: 24 --> Average Loss: 87.303 completed in 250.289 seconds\n",
      "Epoch: 25 --> Average Loss: 86.834 completed in 248.661 seconds\n",
      "Epoch: 26 --> Average Loss: 86.304 completed in 250.428 seconds\n",
      "Epoch: 27 --> Average Loss: 85.712 completed in 249.247 seconds\n",
      "Epoch: 28 --> Average Loss: 85.087 completed in 251.220 seconds\n",
      "Epoch: 29 --> Average Loss: 84.558 completed in 249.313 seconds\n",
      "Epoch: 30 --> Average Loss: 84.042 completed in 256.304 seconds\n",
      "Epoch: 31 --> Average Loss: 83.512 completed in 248.796 seconds\n",
      "Epoch: 32 --> Average Loss: 83.014 completed in 250.137 seconds\n",
      "Epoch: 33 --> Average Loss: 82.599 completed in 250.029 seconds\n",
      "Epoch: 34 --> Average Loss: 82.164 completed in 249.020 seconds\n",
      "Epoch: 35 --> Average Loss: 81.574 completed in 248.932 seconds\n",
      "Epoch: 36 --> Average Loss: 81.043 completed in 250.424 seconds\n",
      "Epoch: 37 --> Average Loss: 80.600 completed in 248.351 seconds\n",
      "Epoch: 38 --> Average Loss: 80.312 completed in 269.794 seconds\n",
      "Epoch: 39 --> Average Loss: 80.001 completed in 297.830 seconds\n",
      "Epoch: 40 --> Average Loss: 79.623 completed in 295.643 seconds\n",
      "Epoch: 41 --> Average Loss: 79.210 completed in 282.722 seconds\n",
      "Epoch: 42 --> Average Loss: 78.802 completed in 283.742 seconds\n",
      "Epoch: 43 --> Average Loss: 78.428 completed in 298.805 seconds\n",
      "Epoch: 44 --> Average Loss: 78.049 completed in 305.012 seconds\n",
      "Epoch: 45 --> Average Loss: 77.764 completed in 313.701 seconds\n",
      "Epoch: 46 --> Average Loss: 77.437 completed in 284.035 seconds\n",
      "Epoch: 47 --> Average Loss: 77.175 completed in 285.102 seconds\n",
      "Epoch: 48 --> Average Loss: 76.859 completed in 279.228 seconds\n",
      "Epoch: 49 --> Average Loss: 76.493 completed in 281.584 seconds\n",
      "Epoch: 50 --> Average Loss: 76.121 completed in 276.119 seconds\n"
     ]
    }
   ],
   "source": [
    "loss_list2 = []\n",
    "train(X_train, y_train, net2, 50, 0.01, loss_list2, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51 --> Average Loss: 78.053 completed in 274.804 seconds\n",
      "Epoch: 52 --> Average Loss: 66.160 completed in 275.425 seconds\n",
      "Epoch: 53 --> Average Loss: 58.597 completed in 276.798 seconds\n",
      "Epoch: 54 --> Average Loss: 52.721 completed in 281.255 seconds\n",
      "Epoch: 55 --> Average Loss: 48.040 completed in 283.450 seconds\n"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, net2, 5, 0.5, loss_list2, 256, cont_from= 50, model_name='net2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56 --> Average Loss: 49.023 completed in 275.124 seconds\n",
      "Epoch: 57 --> Average Loss: 42.169 completed in 275.891 seconds\n",
      "Epoch: 58 --> Average Loss: 35.920 completed in 275.478 seconds\n",
      "Epoch: 59 --> Average Loss: 31.063 completed in 274.577 seconds\n",
      "Epoch: 60 --> Average Loss: 26.835 completed in 280.847 seconds\n"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, net2, 5, 1, loss_list2, 256, cont_from= 55, model_name='net2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61 --> Average Loss: 22.808 completed in 278.146 seconds\n",
      "Epoch: 62 --> Average Loss: 19.825 completed in 274.699 seconds\n",
      "Epoch: 63 --> Average Loss: 17.477 completed in 272.911 seconds\n",
      "Epoch: 64 --> Average Loss: 15.356 completed in 273.119 seconds\n",
      "Epoch: 65 --> Average Loss: 13.424 completed in 282.531 seconds\n",
      "Epoch: 66 --> Average Loss: 11.924 completed in 277.521 seconds\n",
      "Epoch: 67 --> Average Loss: 10.825 completed in 276.487 seconds\n",
      "Epoch: 68 --> Average Loss: 9.842 completed in 273.498 seconds\n",
      "Epoch: 69 --> Average Loss: 9.006 completed in 274.491 seconds\n",
      "Epoch: 70 --> Average Loss: 8.276 completed in 274.001 seconds\n",
      "Epoch: 71 --> Average Loss: 7.719 completed in 273.140 seconds\n",
      "Epoch: 72 --> Average Loss: 7.391 completed in 269.703 seconds\n",
      "Epoch: 73 --> Average Loss: 7.074 completed in 245.423 seconds\n",
      "Epoch: 74 --> Average Loss: 6.710 completed in 244.800 seconds\n",
      "Epoch: 75 --> Average Loss: 6.467 completed in 244.923 seconds\n"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, net2, 15, 1, loss_list2, 256, cont_from= 60, model_name='net2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 76 --> Average Loss: 5.398 completed in 266.715 seconds\n",
      "Epoch: 77 --> Average Loss: 5.272 completed in 263.546 seconds\n",
      "Epoch: 78 --> Average Loss: 5.231 completed in 262.714 seconds\n",
      "Epoch: 79 --> Average Loss: 5.267 completed in 263.097 seconds\n",
      "Epoch: 80 --> Average Loss: 5.250 completed in 265.660 seconds\n",
      "Epoch: 81 --> Average Loss: 5.162 completed in 264.792 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-78b90e45dc8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_list2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcont_from\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m75\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'net2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-115-ac46ce5fc81f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(X, y, net, epochs, lr, loss_list, batch_size, cont_from, model_name, gmin, gmax, time_to_save)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mloss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-f643dd352f90>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, X, y, lr, gmin, gmax)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mgrad_to_backprop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgradW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, net2, 15, 0.5, loss_list2, 256, cont_from= 75, model_name='net2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 82 --> Average Loss: 4.443 completed in 266.221 seconds\n",
      "Epoch: 83 --> Average Loss: 4.322 completed in 278.325 seconds\n",
      "Epoch: 84 --> Average Loss: 4.313 completed in 282.784 seconds\n",
      "Epoch: 85 --> Average Loss: 4.350 completed in 277.627 seconds\n",
      "Epoch: 86 --> Average Loss: 4.379 completed in 278.221 seconds\n"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, net2, 5, 0.1, loss_list2, 256, cont_from= 81, model_name='net2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87 --> Average Loss: 4.283 completed in 277.668 seconds\n",
      "Epoch: 88 --> Average Loss: 4.295 completed in 278.233 seconds\n",
      "Epoch: 89 --> Average Loss: 4.323 completed in 277.695 seconds\n",
      "Epoch: 90 --> Average Loss: 4.354 completed in 281.919 seconds\n",
      "Epoch: 91 --> Average Loss: 4.397 completed in 277.926 seconds\n"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, net2, 5, 0.05, loss_list2, 256, cont_from= 86, model_name='net2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51 --> Average Loss: 34.084 completed in 92.213 seconds\n",
      "Epoch: 52 --> Average Loss: 26.214 completed in 91.261 seconds\n",
      "Epoch: 53 --> Average Loss: 21.716 completed in 92.327 seconds\n",
      "Epoch: 54 --> Average Loss: 18.468 completed in 91.669 seconds\n",
      "Epoch: 55 --> Average Loss: 16.187 completed in 91.825 seconds\n",
      "Epoch: 56 --> Average Loss: 14.386 completed in 92.319 seconds\n",
      "Epoch: 57 --> Average Loss: 12.869 completed in 92.250 seconds\n",
      "Epoch: 58 --> Average Loss: 11.749 completed in 91.595 seconds\n",
      "Epoch: 59 --> Average Loss: 10.847 completed in 92.701 seconds\n",
      "Epoch: 60 --> Average Loss: 10.073 completed in 92.246 seconds\n",
      "Epoch: 61 --> Average Loss: 9.417 completed in 91.963 seconds\n",
      "Epoch: 62 --> Average Loss: 8.851 completed in 93.136 seconds\n",
      "Epoch: 63 --> Average Loss: 8.352 completed in 91.810 seconds\n",
      "Epoch: 64 --> Average Loss: 7.907 completed in 94.203 seconds\n",
      "Epoch: 65 --> Average Loss: 7.539 completed in 91.886 seconds\n",
      "Epoch: 66 --> Average Loss: 7.221 completed in 92.950 seconds\n",
      "Epoch: 67 --> Average Loss: 6.901 completed in 91.682 seconds\n",
      "Epoch: 68 --> Average Loss: 6.654 completed in 92.455 seconds\n",
      "Epoch: 69 --> Average Loss: 6.423 completed in 92.505 seconds\n",
      "Epoch: 70 --> Average Loss: 6.260 completed in 92.199 seconds\n",
      "Epoch: 71 --> Average Loss: 6.085 completed in 91.435 seconds\n",
      "Epoch: 72 --> Average Loss: 5.944 completed in 92.448 seconds\n",
      "Epoch: 73 --> Average Loss: 5.853 completed in 91.845 seconds\n",
      "Epoch: 74 --> Average Loss: 5.749 completed in 92.205 seconds\n",
      "Epoch: 75 --> Average Loss: 5.661 completed in 92.532 seconds\n"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, net, 25, 1, loss_list, 128, cont_from= 50, model_name='net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 76 --> Average Loss: 5.103 completed in 105.149 seconds\n",
      "Epoch: 77 --> Average Loss: 5.034 completed in 102.659 seconds\n",
      "Epoch: 78 --> Average Loss: 5.003 completed in 104.135 seconds\n",
      "Epoch: 79 --> Average Loss: 4.977 completed in 104.175 seconds\n",
      "Epoch: 80 --> Average Loss: 4.949 completed in 104.241 seconds\n",
      "Epoch: 81 --> Average Loss: 4.924 completed in 105.148 seconds\n",
      "Epoch: 82 --> Average Loss: 4.904 completed in 103.714 seconds\n",
      "Epoch: 83 --> Average Loss: 4.890 completed in 103.198 seconds\n",
      "Epoch: 84 --> Average Loss: 4.880 completed in 103.189 seconds\n",
      "Epoch: 85 --> Average Loss: 4.860 completed in 104.215 seconds\n"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, net, 10, 0.5, loss_list, 128, cont_from= 75, model_name='net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 86 --> Average Loss: 5.478 completed in 106.198 seconds\n",
      "Epoch: 87 --> Average Loss: 5.441 completed in 105.384 seconds\n"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, net, 2, 1, loss_list, 128, cont_from= 85, model_name='net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 88 --> Average Loss: 4.693 completed in 120.996 seconds\n",
      "Epoch: 89 --> Average Loss: 4.582 completed in 133.826 seconds\n"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, net, 2, 0.1, loss_list, 128, cont_from= 87, model_name='net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-253e282daea7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcont_from\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m89\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'net'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-115-ac46ce5fc81f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(X, y, net, epochs, lr, loss_list, batch_size, cont_from, model_name, gmin, gmax, time_to_save)\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_calc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-f643dd352f90>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-de8ecc04c2f2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtilda_inp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mgamma_inp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconc_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mgammau\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma_inp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma_inp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, net, 10, 0.1, loss_list, 128, cont_from= 89, model_name='net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = one_hot_creator(X_train[:128], y_train[:128])\n",
    "p = net.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted the actual being\n",
      "predicted a actual run\n",
      "predicted <EOS> actual by\n",
      "predicted <EOS> actual the\n",
      "predicted <EOS> actual head\n",
      "predicted flew actual of\n",
      "predicted <EOS> actual an\n",
      "predicted used actual investment\n",
      "predicted <EOS> actual firm\n",
      "predicted <EOS> actual <EOS>\n",
      "predicted <EOS> actual he\n",
      "predicted <EOS> actual was\n",
      "predicted <EOS> actual manipulating\n",
      "predicted <EOS> actual the\n",
      "predicted <EOS> actual market\n",
      "predicted <EOS> actual with\n",
      "predicted of actual his\n",
      "predicted <EOS> actual bombing\n",
      "predicted <EOS> actual targets\n",
      "predicted <EOS> actual <EOS>\n",
      "predicted <EOS> actual engineer\n",
      "predicted <EOS> actual asi\n",
      "predicted <EOS> actual ali\n",
      "predicted <EOS> actual from\n",
      "predicted and actual tikrit\n",
      "predicted <EOS> actual <EOS>\n",
      "predicted <EOS> actual also\n",
      "predicted <EOS> actual killed\n",
      "predicted was actual in\n",
      "predicted <EOS> actual the\n",
      "predicted so actual attack\n",
      "predicted <EOS> actual <EOS>\n",
      "predicted morality actual it\n",
      "predicted <EOS> actual had\n",
      "predicted <EOS> actual no\n",
      "predicted <EOS> actual legal\n",
      "predicted <EOS> actual grounds\n",
      "predicted <EOS> actual for\n",
      "predicted <EOS> actual such\n",
      "predicted <EOS> actual an\n",
      "predicted <EOS> actual exclusion\n",
      "predicted <EOS> actual <EOS>\n",
      "predicted <EOS> actual a\n",
      "predicted <EOS> actual small\n",
      "predicted <EOS> actual one\n",
      "predicted <EOS> actual and\n",
      "predicted <EOS> actual easily\n",
      "predicted <EOS> actual missed\n",
      "predicted do actual <EOS>\n",
      "predicted <EOS> actual view\n",
      "predicted she actual it\n",
      "predicted <EOS> actual is\n",
      "predicted <EOS> actual highly\n",
      "predicted <EOS> actual significant\n",
      "predicted <EOS> actual <EOS>\n",
      "predicted <EOS> actual iraq\n",
      "predicted <EOS> actual is\n",
      "predicted <EOS> actual only\n",
      "predicted the actual going\n",
      "predicted in actual to\n",
      "predicted <EOS> actual get\n",
      "predicted rooms actual better\n",
      "predicted <EOS> actual this\n",
      "predicted <EOS> actual way\n",
      "predicted <EOS> actual <EOS>\n",
      "predicted this actual a\n",
      "predicted <EOS> actual crime\n",
      "predicted your actual against\n",
      "predicted but actual humanity\n",
      "predicted <EOS> actual prosecute\n",
      "predicted <EOS> actual the\n",
      "predicted <EOS> actual person\n",
      "predicted <EOS> actual <EOS>\n",
      "predicted <EOS> actual the\n",
      "predicted <EOS> actual court\n",
      "predicted the actual managed\n",
      "predicted <EOS> actual to\n",
      "predicted <EOS> actual take\n",
      "predicted <EOS> actual his\n",
      "predicted <EOS> actual deposition\n",
      "predicted <EOS> actual before\n",
      "predicted <EOS> actual he\n",
      "predicted <EOS> actual died\n",
      "predicted <EOS> actual <EOS>\n",
      "predicted <EOS> actual again\n",
      "predicted <EOS> actual nov28\n",
      "predicted <EOS> actual <EOS>\n",
      "predicted <EOS> actual talk\n",
      "predicted in actual that\n",
      "predicted <EOS> actual the\n",
      "predicted have actual night\n",
      "predicted <EOS> actual curfew\n",
      "predicted and actual might\n",
      "predicted <EOS> actual be\n",
      "predicted <EOS> actual implemented\n",
      "predicted a actual again\n",
      "predicted <EOS> actual <EOS>\n",
      "predicted <EOS> actual are\n",
      "predicted <EOS> actual reported\n",
      "predicted excellent actual dead\n",
      "predicted <EOS> actual and\n",
      "predicted <EOS> actual 500\n",
      "predicted <EOS> actual reported\n",
      "predicted <EOS> actual wounded\n",
      "predicted no actual in\n",
      "predicted <EOS> actual fallujah\n",
      "predicted <EOS> actual alone\n",
      "predicted <EOS> actual <EOS>\n",
      "predicted but actual that\n",
      "predicted <EOS> actual icdc\n",
      "predicted <EOS> actual were\n",
      "predicted <EOS> actual controlling\n",
      "predicted <EOS> actual ramadi\n",
      "predicted <EOS> actual <EOS>\n",
      "predicted the actual tour\n",
      "predicted <EOS> actual chernobyl\n",
      "predicted <EOS> actual and\n",
      "predicted <EOS> actual write\n",
      "predicted <EOS> actual your\n",
      "predicted <EOS> actual own\n",
      "predicted <EOS> actual story\n",
      "predicted in actual <EOS>\n",
      "predicted <EOS> actual a\n",
      "predicted <EOS> actual post\n",
      "predicted <EOS> actual about\n",
      "predicted <EOS> actual faultfinding\n",
      "predicted <EOS> actual or\n",
      "predicted <EOS> actual assigning\n"
     ]
    }
   ],
   "source": [
    "yint = np.argmax(y, axis = 1)\n",
    "pint = np.argmax(p, axis = 1)\n",
    "for i in range(pint.shape[0]):\n",
    "    print(\"predicted \" + str(reverse_vocab[pint[i]]) + \" actual \" + str(reverse_vocab[yint[i]]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 --> Average Loss: 0.250\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "j = 0.25\n",
    "print(\"Epoch: %d --> Average Loss: %.3f\" %(i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in range(1,10):\n",
    "    l.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 5\n",
    "with open('param_' + str(j) + 'pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('param' + j + '.pkl', 'rb') as f:\n",
    "    tada = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6683"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildmodel(VOC_LEN, inp_len = input_len):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, input_shape = (inp_len, VOC_LEN), return_sequences = True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(VOC_LEN, activation = 'softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = buildmodel(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_3 to have 2 dimensions, but got array with shape (1000, 6683, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-4a2622fcfdb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_oh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_oh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Sem8/flipkart/fpenv/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Sem8/flipkart/fpenv/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Sem8/flipkart/fpenv/lib/python3.5/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_3 to have 2 dimensions, but got array with shape (1000, 6683, 1)"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_oh, y_train_oh, epochs = 50, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fpenv",
   "language": "python",
   "name": "fpenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
