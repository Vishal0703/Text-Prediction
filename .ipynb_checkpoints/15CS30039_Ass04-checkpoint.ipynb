{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing various modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Creating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_document(doco):\n",
    "    punctuation = string.punctuation + '\\n\\n'\n",
    "    punc_replace = ''.join([' ' for s in punctuation])\n",
    "    doco_clean = doco.replace('-', ' ')\n",
    "    doco_alphas = re.sub(r'\\W +', '', doco_clean)\n",
    "    trans_table = str.maketrans(punctuation, punc_replace)\n",
    "    doco_clean = ' '.join([word.translate(trans_table) for word in doco_alphas.split(' ')])\n",
    "    doco_clean = doco_clean.split(' ')\n",
    "    doco_clean = [word.lower() for word in doco_clean if len(word) > 0]\n",
    "    \n",
    "    return doco_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_creator(file, inp_len):\n",
    "    X = []\n",
    "    label = []\n",
    "    for line in file:\n",
    "        cline = clean_document(line)\n",
    "        length = len(cline)\n",
    "        if length <= inp_len:\n",
    "            continue\n",
    "        for i in range(0, length - inp_len):\n",
    "            X.append(cline[i:i+inp_len])\n",
    "            label.append(cline[i+inp_len])\n",
    "        X.append(cline[i+1:])\n",
    "        label.append('<EOS>')\n",
    "    return X, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_train = open('train.txt')\n",
    "file_test = open('test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_temp, y_train_temp = dataset_creator(file_train, timesteps)\n",
    "X_test, y_test = dataset_creator(file_test, timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a balanced dataset and taking only 200 cases of most frequent terms like 'the', 'EOS', 'a', 'of' etc\n",
    "eos = 0\n",
    "the = 0\n",
    "to = 0\n",
    "andc = 0\n",
    "a = 0\n",
    "of = 0\n",
    "inc = 0\n",
    "forc = 0\n",
    "you = 0\n",
    "isc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.random.choice(len(y_train_temp), size = len(y_train_temp), replace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = []\n",
    "X_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ind:\n",
    "    if y_train_temp[i] == '<EOS>':\n",
    "        eos += 1\n",
    "        if eos <= 200:\n",
    "            y_train.append(y_train_temp[i])\n",
    "            X_train.append(X_train_temp[i])\n",
    "            \n",
    "    elif y_train_temp[i] == 'the':\n",
    "        the += 1\n",
    "        if the <= 200:\n",
    "            y_train.append(y_train_temp[i])\n",
    "            X_train.append(X_train_temp[i])\n",
    "            \n",
    "    elif y_train_temp[i] == 'to':\n",
    "        to += 1\n",
    "        if to <= 200:\n",
    "            y_train.append(y_train_temp[i])\n",
    "            X_train.append(X_train_temp[i])\n",
    "            \n",
    "    elif y_train_temp[i] == 'and':\n",
    "        andc += 1\n",
    "        if andc <= 200:\n",
    "            y_train.append(y_train_temp[i])\n",
    "            X_train.append(X_train_temp[i])\n",
    "            \n",
    "    elif y_train_temp[i] == 'a':\n",
    "        a += 1\n",
    "        if a <= 200:\n",
    "            y_train.append(y_train_temp[i])\n",
    "            X_train.append(X_train_temp[i])\n",
    "            \n",
    "    elif y_train_temp[i] == 'of':\n",
    "        of += 1\n",
    "        if of <= 200:\n",
    "            y_train.append(y_train_temp[i])\n",
    "            X_train.append(X_train_temp[i])\n",
    "            \n",
    "    elif y_train_temp[i] == 'in':\n",
    "        inc += 1\n",
    "        if inc <= 200:\n",
    "            y_train.append(y_train_temp[i])\n",
    "            X_train.append(X_train_temp[i])\n",
    "            \n",
    "    elif y_train_temp[i] == 'for':\n",
    "        forc += 1\n",
    "        if forc <= 200:\n",
    "            y_train.append(y_train_temp[i])\n",
    "            X_train.append(X_train_temp[i])\n",
    "            \n",
    "    elif y_train_temp[i] == 'you':\n",
    "        you += 1\n",
    "        if you <= 200:\n",
    "            y_train.append(y_train_temp[i])\n",
    "            X_train.append(X_train_temp[i])\n",
    "            \n",
    "    elif y_train_temp[i] == 'is':\n",
    "        isc += 1\n",
    "        if isc <= 200:\n",
    "            y_train.append(y_train_temp[i])\n",
    "            X_train.append(X_train_temp[i])\n",
    "            \n",
    "    else:\n",
    "        y_train.append(y_train_temp[i])\n",
    "        X_train.append(X_train_temp[i])\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating vocabulary and reverse vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "i = 0\n",
    "for row in X_train_temp:\n",
    "    for el in row:\n",
    "        if el not in vocab:\n",
    "            vocab[el] = i\n",
    "            i += 1\n",
    "            \n",
    "for row in X_test:\n",
    "    for el in row:\n",
    "        if el not in vocab:\n",
    "            vocab[el] = i\n",
    "            i += 1\n",
    "vocab['<EOS>'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_vocab = {}\n",
    "for key in vocab.keys():\n",
    "    reverse_vocab[vocab[key]] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_creator(X,y, vocab = vocab, input_len = timesteps):\n",
    "    vocab_len = len(vocab)\n",
    "    one_hotX = []\n",
    "    one_hotY = []\n",
    "    for row in X:\n",
    "        temp = np.zeros(shape = (input_len, vocab_len))\n",
    "        for (i,el) in enumerate(row):\n",
    "            if el in vocab:\n",
    "                temp[i][vocab[el]] = 1\n",
    "            else:\n",
    "                temp[i][vocab['<EOS>']] = 1\n",
    "        one_hotX.append(temp)\n",
    "            \n",
    "    for row in y:\n",
    "        temp = np.zeros(shape = (vocab_len,))\n",
    "        if row in vocab:\n",
    "            temp[vocab[row]] = 1\n",
    "        else:\n",
    "            temp[vocab['<EOS>']] = 1\n",
    "        one_hotY.append(temp)\n",
    "        \n",
    "    return np.array(one_hotX), np.array(one_hotY)    \n",
    "\n",
    "def ft_embed(X,y,vocab2 = vocab, vocab = None):\n",
    "    vocab_len = len(vocab)\n",
    "    ft_embedX = []\n",
    "    one_hotY = []\n",
    "    for row in X:\n",
    "        temp = []\n",
    "        for el in row:\n",
    "            if el in vocab:\n",
    "                temp.append(vocab[el])\n",
    "            else:\n",
    "                temp.append(vocab['<EOS>'])\n",
    "        ft_embedX.append(np.array(temp))\n",
    "        \n",
    "    for row in y:\n",
    "        temp = np.zeros(shape = (vocab_len,))\n",
    "        if row in vocab2:\n",
    "            temp[vocab2[row]] = 1\n",
    "        else:\n",
    "            temp[vocab2['<EOS>']] = 1\n",
    "        one_hotY.append(temp)\n",
    "        \n",
    "    return np.array(ft_embedX), np.array(one_hotY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_oh, y_train_oh = ft_embed(X_train[:128], y_train[:128], vocab= vocab_ft)   #one_hot vectors\n",
    "#X_test_oh, y_test_oh = one_hot_creator(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU Unit - Forward and Backprop Written "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU():\n",
    "    def __init__(self,hidden_units, embed_len, batch_size, timesteps):\n",
    "        \n",
    "        self.hidden_units = hidden_units\n",
    "        self.Wc = np.random.normal(size = (embed_len + self.hidden_units, self.hidden_units)) \n",
    "        self.Wu = np.random.normal(size = (embed_len + self.hidden_units, self.hidden_units)) \n",
    "        self.bc = np.random.normal(size = (1, self.hidden_units)) \n",
    "        self.bu = np.random.normal(size = (1, self.hidden_units))         \n",
    "        self.batch_size = batch_size\n",
    "        self.clist = []\n",
    "        self.glist = []\n",
    "        self.tlist = []\n",
    "        self.c_initial = np.zeros(shape = (self.batch_size, self.hidden_units))\n",
    "        self.timesteps = timesteps\n",
    "        \n",
    "    def forward(self,X, ts = None):\n",
    "        if ts is None:\n",
    "            ts = self.timesteps\n",
    "        c = self.c_initial\n",
    "        for i in range(ts):\n",
    "            conc_inp = np.concatenate((X[:,i,:], c), axis = 1)\n",
    "            \n",
    "            tilda_inp = np.dot(conc_inp, self.Wc) + self.bc\n",
    "            c_tilda = self.tanh(tilda_inp)\n",
    "            self.tlist.append(tilda_inp)\n",
    "            \n",
    "            gamma_inp = np.dot(conc_inp, self.Wu) + self.bu\n",
    "            gammau = self.sigmoid(gamma_inp)\n",
    "            self.glist.append(gamma_inp)\n",
    "            \n",
    "            c = np.multiply(gammau, c_tilda) + np.multiply(1-gammau, c)\n",
    "            self.clist.append(c)\n",
    "        \n",
    "        return c  \n",
    "    \n",
    "    def backward(self, prev_dev, X, lr, gmin, gmax, ts = None):\n",
    "        if ts is None:\n",
    "            ts = self.timesteps\n",
    "        for i in range(ts):\n",
    "            ind = self.timesteps - 1 - i\n",
    "            if ind > 0:\n",
    "                conc_inp = np.concatenate((X[:,ind,:], self.clist[ind-1]), axis = 1)\n",
    "            else:\n",
    "                conc_inp = np.concatenate((X[:,ind,:], self.c_initial), axis = 1)\n",
    "            \n",
    "            gammau = self.sigmoid(self.glist[ind])\n",
    "            \n",
    "            tanderv = 1 - (self.tanh(self.tlist[ind]))**2\n",
    "            sigmaderv = gammau*(1-gammau)\n",
    "            \n",
    "            c_tilda = self.tanh(self.tlist[ind])\n",
    "            inp_transpose = np.transpose(conc_inp)\n",
    "            \n",
    "            temp = np.multiply(gammau, tanderv)\n",
    "            temp = np.multiply(temp, prev_dev)\n",
    "            \n",
    "            gradWc = np.clip(np.dot(inp_transpose, temp)/self.batch_size, gmin, gmax)\n",
    "            gradbc = np.clip(np.sum(temp, axis = 0)/self.batch_size, gmin, gmax)\n",
    "            \n",
    "            if ind > 0:\n",
    "                temp = np.multiply(c_tilda - self.clist[ind - 1], sigmaderv)\n",
    "            else:\n",
    "                temp = np.multiply(c_tilda - self.c_initial, sigmaderv)\n",
    "            \n",
    "            temp = np.multiply(temp, prev_dev) \n",
    "            \n",
    "            gradWu = np.clip(np.dot(inp_transpose, temp)/self.batch_size, gmin, gmax)\n",
    "            gradbu = np.clip(np.sum(temp, axis = 0)/self.batch_size, gmin, gmax)\n",
    "            \n",
    "            self.Wc -= lr*gradWc\n",
    "            self.bc -= lr*gradbc\n",
    "            self.Wu -= lr*gradWu\n",
    "            self.bu -= lr*gradbu\n",
    "        \n",
    "        self.clist = []\n",
    "        self.tlist = []\n",
    "        self.glist = []\n",
    "        \n",
    "    def sigmoid(self, X):\n",
    "        return ( 1/ (1 + np.exp(-X)))\n",
    "    \n",
    "    def tanh(self, X):\n",
    "        p = np.exp(X)\n",
    "        m = np.exp(-X)\n",
    "        return((p-m)/(p+m))  \n",
    "    \n",
    "    def load_param(self, param):\n",
    "        self.Wc = param['Wc']\n",
    "        self.bc = param['bc']\n",
    "        self.Wu = param['Wu']\n",
    "        self.bu = param['bu']\n",
    "        \n",
    "    def save_param(self, param):\n",
    "        param['Wc'] = self.Wc\n",
    "        param['bc'] = self.bc\n",
    "        param['Wu'] = self.Wu\n",
    "        param['bu'] = self.bu\n",
    "        return param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network():\n",
    "    def __init__(self, hidden_units, embed_len, output_size, batch_size, timesteps):\n",
    "        self.gru = GRU(hidden_units, embed_len, batch_size, timesteps)\n",
    "        self.W = np.random.normal(size = (hidden_units, output_size))\n",
    "        self.b = np.random.normal(size = (1, output_size))\n",
    "        self.batch_size = batch_size\n",
    "        self.timesteps = timesteps\n",
    "        self.c = 0\n",
    "        self.o = 0\n",
    "        \n",
    "    def forward(self, X, ts = None):\n",
    "        if ts is None:\n",
    "            ts = self.timesteps\n",
    "        self.c = self.gru.forward(X, ts)\n",
    "        self.o = self.softmax(np.dot(self.c, self.W) + self.b)\n",
    "        return self.o\n",
    "    \n",
    "    def backward(self, X, y, lr, gmin, gmax,ts = None):\n",
    "        if ts is None:\n",
    "            ts = self.timesteps\n",
    "            \n",
    "        grad = self.o - y\n",
    "\n",
    "        gradW = np.clip(np.dot(np.transpose(self.c), grad)/self.batch_size, gmin, gmax)\n",
    "        gradb = np.clip(np.sum(grad, axis = 0)/self.batch_size, gmin, gmax)\n",
    "\n",
    "        self.c = 0\n",
    "        self.o = 0\n",
    "                        \n",
    "        grad_to_backprop = np.dot(grad, np.transpose(self.W))\n",
    "        \n",
    "        self.W -= lr*gradW\n",
    "        self.b -= lr*gradb\n",
    "        \n",
    "        self.gru.backward(grad_to_backprop, X, lr, gmin, gmax)\n",
    "        \n",
    "    def softmax(self, X):\n",
    "        exps = np.exp(X - np.reshape(np.max(X, axis = 1), (X.shape[0], 1)))\n",
    "        return exps / np.reshape(np.sum(exps, axis = 1), (X.shape[0], 1))\n",
    "    \n",
    "    def load_param(self, param):\n",
    "        self.W = param['W']\n",
    "        self.b = param['b']\n",
    "        self.gru.load_param(param)\n",
    "        \n",
    "    def save_param(self):\n",
    "        param = {}\n",
    "        param['W'] = self.W\n",
    "        param['b'] = self.b\n",
    "        return(self.gru.save_param(param))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_calc(pred, actual):\n",
    "    mult = np.multiply(np.log(pred), actual)\n",
    "    return -np.sum(mult)/pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_create(X,y,embed_type, vocab_ft = None):\n",
    "    if embed_type == 'one_hot':\n",
    "        return one_hot_creator(X,y)\n",
    "    else:\n",
    "        return ft_embed(X,y,vocab = vocab_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, net, epochs, lr, loss_list, batch_size, cont_from = 0, model_name = 'model', gmin = -10, gmax = 10, time_to_save = 1, embed_type = 'one_hot', vocab_ft = None):\n",
    "    train_len = len(X)\n",
    "    for i in range(1, epochs+1):\n",
    "        tic = time.time()\n",
    "        counter = 0\n",
    "        \n",
    "        for j in range(0, train_len, batch_size):\n",
    "            \n",
    "            if(j+batch_size > train_len):\n",
    "                X_batch, y_batch = embed_create(X[train_len-batch_size:], y[train_len-batch_size:], embed_type, vocab_ft)\n",
    "            else:\n",
    "                X_batch, y_batch = embed_create(X[j:j+batch_size], y[j:j+batch_size], embed_type, vocab_ft)\n",
    "                \n",
    "            pred = net.forward(X_batch)\n",
    "            loss = loss_calc(pred, y_batch)\n",
    "            \n",
    "            loss_list.append(loss)\n",
    "            net.backward(X_batch, y_batch, lr, gmin, gmax)\n",
    "            counter += 1\n",
    "            \n",
    "        if (cont_from + i) % time_to_save == 0:\n",
    "            param_dict = net.save_param()\n",
    "            with open('param_epoch_' + model_name + \"_\" + str(cont_from + i) + '.pkl', 'wb') as f:\n",
    "                pickle.dump(param_dict, f)\n",
    "        \n",
    "        ep_time = time.time() - tic\n",
    "        print(\"Epoch: %d --> Average Loss: %.3f completed in %.3f seconds\" \n",
    "              %(cont_from + i, sum(loss_list[len(loss_list)-counter:]) / counter, ep_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_new = Network(256, len(vocab), len(vocab), 128, timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 --> Average Loss: 24.237 completed in 100.881 seconds\n",
      "Epoch: 2 --> Average Loss: 9.280 completed in 90.495 seconds\n",
      "Epoch: 3 --> Average Loss: 8.375 completed in 110.736 seconds\n",
      "Epoch: 4 --> Average Loss: 8.116 completed in 143.083 seconds\n",
      "Epoch: 5 --> Average Loss: 7.966 completed in 125.172 seconds\n",
      "Epoch: 6 --> Average Loss: 7.860 completed in 124.362 seconds\n",
      "Epoch: 7 --> Average Loss: 7.778 completed in 127.153 seconds\n",
      "Epoch: 8 --> Average Loss: 7.712 completed in 141.150 seconds\n",
      "Epoch: 9 --> Average Loss: 7.656 completed in 167.509 seconds\n",
      "Epoch: 10 --> Average Loss: 7.606 completed in 169.247 seconds\n",
      "Epoch: 11 --> Average Loss: 7.563 completed in 167.159 seconds\n",
      "Epoch: 12 --> Average Loss: 7.524 completed in 146.361 seconds\n",
      "Epoch: 13 --> Average Loss: 7.488 completed in 118.750 seconds\n",
      "Epoch: 14 --> Average Loss: 7.455 completed in 107.928 seconds\n",
      "Epoch: 15 --> Average Loss: 7.425 completed in 90.009 seconds\n"
     ]
    }
   ],
   "source": [
    "loss_list_new = []\n",
    "train(X_train, y_train, net_new, 15, 1, loss_list_new, 128, model_name='net_new', time_to_save=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 --> Average Loss: 7.403 completed in 83.685 seconds\n",
      "Epoch: 17 --> Average Loss: 7.389 completed in 87.236 seconds\n",
      "Epoch: 18 --> Average Loss: 7.376 completed in 85.717 seconds\n",
      "Epoch: 19 --> Average Loss: 7.362 completed in 98.273 seconds\n",
      "Epoch: 20 --> Average Loss: 7.350 completed in 92.828 seconds\n"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, net_new, 5, 0.5, loss_list_new, 128, cont_from= 15, model_name='net_new', time_to_save=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 --> Average Loss: 7.321 completed in 99.160 seconds\n",
      "Epoch: 22 --> Average Loss: 7.305 completed in 84.296 seconds\n",
      "Epoch: 23 --> Average Loss: 7.282 completed in 86.016 seconds\n",
      "Epoch: 24 --> Average Loss: 7.260 completed in 83.819 seconds\n",
      "Epoch: 25 --> Average Loss: 7.239 completed in 89.671 seconds\n"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, net_new, 5, 1, loss_list_new, 128, cont_from= 20, model_name='net_new', time_to_save=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 --> Average Loss: 7.196 completed in 88.926 seconds\n"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, net_new, 1, 2, loss_list_new, 128, cont_from= 25, model_name='net_new', time_to_save=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 --> Average Loss: 7.168 completed in 84.341 seconds\n",
      "Epoch: 28 --> Average Loss: 7.132 completed in 96.998 seconds\n",
      "Epoch: 29 --> Average Loss: 7.098 completed in 106.015 seconds\n",
      "Epoch: 30 --> Average Loss: 7.065 completed in 107.353 seconds\n",
      "Epoch: 31 --> Average Loss: 7.032 completed in 81.742 seconds\n"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, net_new, 5, 2, loss_list_new, 128, cont_from= 26, model_name='net_new', time_to_save=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading Network parameters\n",
    "\n",
    "# with open('param_epoch_net_new_30.pkl', 'rb') as f:\n",
    "#     tada = pickle.load(f)\n",
    "# net_new.load_param(tada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max(arr, window):\n",
    "    return np.argsort(arr)[-window:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred,actual, window):\n",
    "    l = []\n",
    "    for el in pred:\n",
    "        l.append(find_max(el, window))\n",
    "    \n",
    "    yint = np.argmax(actual, axis = 1)\n",
    "    acc = 0\n",
    "    \n",
    "    for i,el in enumerate(l):\n",
    "        if yint[i] in el:\n",
    "            acc += 1\n",
    "            \n",
    "    return acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tester(X, y, net, batch_size, embed_type='one_hot', vocab_ft = None, window = 5):\n",
    "    test_len = len(X)\n",
    "    counter = 0\n",
    "    accumulate = 0\n",
    "    loss = 0 \n",
    "    for j in range(0, test_len, batch_size):\n",
    "            counter += 1\n",
    "            if(j+batch_size > test_len):\n",
    "                X_batch, y_batch = embed_create(X[test_len-batch_size:], y[test_len-batch_size:], embed_type, vocab_ft)\n",
    "            else:\n",
    "                X_batch, y_batch = embed_create(X[j:j+batch_size], y[j:j+batch_size], embed_type, vocab_ft)\n",
    "                \n",
    "            pred = net.forward(X_batch)\n",
    "            loss += loss_calc(pred, y_batch)*batch_size\n",
    "            \n",
    "            accumulate += accuracy(pred, y_batch, window)\n",
    "            \n",
    "    print(\"Loss is %.3f and Accuracy for window size %d is %.3f \" \n",
    "          %(loss /(counter*batch_size), window, 100*accumulate/(counter*batch_size)))\n",
    "            \n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Accuracy ( One-hot )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task1: For Train set using one-hot\n",
      "Loss is 7.126 and Accuracy for window size 5 is 10.608 \n"
     ]
    }
   ],
   "source": [
    "print(\"Task1: For Train set using one-hot\")\n",
    "tester(X_train, y_train, net_new, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task1: For Test set using one-hot\n",
      "Loss is 7.291 and Accuracy for window size 5 is 3.794 \n"
     ]
    }
   ],
   "source": [
    "print(\"Task1: For Test set using one-hot\")\n",
    "tester(X_test, y_test, net_new, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Fast Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_fasttext = open('fasttext_data.txt', \"w\")\n",
    "count = 0\n",
    "for key in vocab.keys():\n",
    "    file_fasttext.write(key)\n",
    "    count += 1\n",
    "    if count == 10:\n",
    "        file_fasttext.write(\"\\n\")\n",
    "        count = 0\n",
    "        continue\n",
    "    file_fasttext.write(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.skipgram('fasttext_data.txt', 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.load_model('model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_ft = {}\n",
    "for row in X_train_temp:\n",
    "    for el in row:\n",
    "        if el not in vocab_ft:\n",
    "            vocab_ft[el] = model[el]\n",
    "            i += 1\n",
    "            \n",
    "for row in X_test:\n",
    "    for el in row:\n",
    "        if el not in vocab_ft:\n",
    "            vocab_ft[el] = model[el]\n",
    "            i += 1\n",
    "vocab_ft['<EOS>'] = model['<EOS>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_ft = Network(256, 100, len(vocab), 128, timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 --> Average Loss: 24.561 completed in 35.432 seconds\n",
      "Epoch: 2 --> Average Loss: 15.106 completed in 35.501 seconds\n",
      "Epoch: 3 --> Average Loss: 11.789 completed in 37.275 seconds\n",
      "Epoch: 4 --> Average Loss: 10.330 completed in 36.917 seconds\n",
      "Epoch: 5 --> Average Loss: 9.850 completed in 33.803 seconds\n",
      "Epoch: 6 --> Average Loss: 9.726 completed in 33.815 seconds\n",
      "Epoch: 7 --> Average Loss: 9.961 completed in 35.073 seconds\n",
      "Epoch: 8 --> Average Loss: 9.299 completed in 34.299 seconds\n",
      "Epoch: 9 --> Average Loss: 9.382 completed in 35.244 seconds\n",
      "Epoch: 10 --> Average Loss: 9.292 completed in 33.752 seconds\n",
      "Epoch: 11 --> Average Loss: 9.138 completed in 33.757 seconds\n",
      "Epoch: 12 --> Average Loss: 8.854 completed in 33.749 seconds\n",
      "Epoch: 13 --> Average Loss: 9.716 completed in 33.818 seconds\n",
      "Epoch: 14 --> Average Loss: 8.960 completed in 34.117 seconds\n",
      "Epoch: 15 --> Average Loss: 9.703 completed in 33.976 seconds\n"
     ]
    }
   ],
   "source": [
    "loss_list_ft = []\n",
    "train(X_train, y_train, net_ft, 15, 1, loss_list_ft, 128, model_name='net_ft', time_to_save=5, embed_type='ft', vocab_ft=vocab_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 --> Average Loss: 8.909 completed in 34.211 seconds\n",
      "Epoch: 17 --> Average Loss: 8.639 completed in 35.102 seconds\n",
      "Epoch: 18 --> Average Loss: 8.832 completed in 34.498 seconds\n",
      "Epoch: 19 --> Average Loss: 8.806 completed in 33.328 seconds\n",
      "Epoch: 20 --> Average Loss: 8.579 completed in 33.838 seconds\n",
      "Epoch: 21 --> Average Loss: 8.641 completed in 33.988 seconds\n",
      "Epoch: 22 --> Average Loss: 8.534 completed in 34.277 seconds\n",
      "Epoch: 23 --> Average Loss: 8.900 completed in 33.853 seconds\n",
      "Epoch: 24 --> Average Loss: 8.840 completed in 34.239 seconds\n",
      "Epoch: 25 --> Average Loss: 8.497 completed in 34.561 seconds\n"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, net_ft, 10, 1, loss_list_ft, 128, cont_from = 15, model_name='net_ft', time_to_save=5, embed_type='ft', vocab_ft=vocab_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 --> Average Loss: 7.624 completed in 34.956 seconds\n",
      "Epoch: 27 --> Average Loss: 7.531 completed in 34.186 seconds\n",
      "Epoch: 28 --> Average Loss: 7.434 completed in 33.629 seconds\n",
      "Epoch: 29 --> Average Loss: 7.525 completed in 33.307 seconds\n",
      "Epoch: 30 --> Average Loss: 7.442 completed in 33.802 seconds\n",
      "Epoch: 31 --> Average Loss: 7.521 completed in 33.994 seconds\n",
      "Epoch: 32 --> Average Loss: 7.437 completed in 33.353 seconds\n",
      "Epoch: 33 --> Average Loss: 7.518 completed in 34.213 seconds\n",
      "Epoch: 34 --> Average Loss: 7.429 completed in 36.682 seconds\n",
      "Epoch: 35 --> Average Loss: 7.516 completed in 34.633 seconds\n"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, net_ft, 10, 0.5, loss_list_ft, 128, cont_from = 25, model_name='net_ft', time_to_save=5, embed_type='ft', vocab_ft=vocab_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 --> Average Loss: 7.413 completed in 31.832 seconds\n",
      "Epoch: 37 --> Average Loss: 7.490 completed in 32.313 seconds\n",
      "Epoch: 38 --> Average Loss: 7.478 completed in 31.427 seconds\n",
      "Epoch: 39 --> Average Loss: 7.511 completed in 33.625 seconds\n",
      "Epoch: 40 --> Average Loss: 7.413 completed in 33.131 seconds\n"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, net_ft, 5, 0.5, loss_list_ft, 128, cont_from = 35, model_name='net_ft', time_to_save=5, embed_type='ft', vocab_ft=vocab_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41 --> Average Loss: 7.502 completed in 41.957 seconds\n",
      "Epoch: 42 --> Average Loss: 7.432 completed in 36.503 seconds\n",
      "Epoch: 43 --> Average Loss: 7.534 completed in 36.062 seconds\n",
      "Epoch: 44 --> Average Loss: 7.471 completed in 37.717 seconds\n",
      "Epoch: 45 --> Average Loss: 7.507 completed in 33.013 seconds\n",
      "Epoch: 46 --> Average Loss: 7.389 completed in 32.726 seconds\n",
      "Epoch: 47 --> Average Loss: 7.474 completed in 32.748 seconds\n",
      "Epoch: 48 --> Average Loss: 7.505 completed in 31.120 seconds\n",
      "Epoch: 49 --> Average Loss: 7.386 completed in 32.400 seconds\n",
      "Epoch: 50 --> Average Loss: 7.415 completed in 36.221 seconds\n"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, net_ft, 10, 0.5, loss_list_ft, 128, cont_from = 40, model_name='net_ft', time_to_save=5, embed_type='ft', vocab_ft=vocab_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51 --> Average Loss: 7.262 completed in 35.078 seconds\n",
      "Epoch: 52 --> Average Loss: 7.219 completed in 33.299 seconds\n",
      "Epoch: 53 --> Average Loss: 7.217 completed in 39.256 seconds\n",
      "Epoch: 54 --> Average Loss: 7.216 completed in 43.801 seconds\n",
      "Epoch: 55 --> Average Loss: 7.215 completed in 41.157 seconds\n"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, net_ft, 5, 0.1, loss_list_ft, 128, cont_from = 50, model_name='net_ft', time_to_save=5, embed_type='ft', vocab_ft=vocab_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56 --> Average Loss: 7.310 completed in 43.103 seconds\n",
      "Epoch: 57 --> Average Loss: 7.256 completed in 44.069 seconds\n",
      "Epoch: 58 --> Average Loss: 7.238 completed in 41.757 seconds\n",
      "Epoch: 59 --> Average Loss: 7.233 completed in 40.973 seconds\n",
      "Epoch: 60 --> Average Loss: 7.232 completed in 40.473 seconds\n"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, net_ft, 5, 0.01, loss_list_ft, 128, cont_from = 55, model_name='net_ft', time_to_save=5, embed_type='ft', vocab_ft=vocab_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61 --> Average Loss: 7.238 completed in 37.845 seconds\n",
      "Epoch: 62 --> Average Loss: 7.237 completed in 31.341 seconds\n",
      "Epoch: 63 --> Average Loss: 7.236 completed in 35.438 seconds\n",
      "Epoch: 64 --> Average Loss: 7.235 completed in 33.357 seconds\n",
      "Epoch: 65 --> Average Loss: 7.234 completed in 32.324 seconds\n"
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, net_ft, 5, 0.001, loss_list_ft, 128, cont_from = 60, model_name='net_ft', time_to_save=5, embed_type='ft', vocab_ft=vocab_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading Network parameters\n",
    "\n",
    "# with open('param_epoch_net_ft_65.pkl', 'rb') as f:\n",
    "#     tada = pickle.load(f)\n",
    "# net_ft.load_param(tada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Accuracy ( Fast Text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task1: For Train set using Fast Text\n",
      "Loss is 7.238 and Accuracy for window size 5 is 5.149 \n"
     ]
    }
   ],
   "source": [
    "print(\"Task1: For Train set using Fast Text\")\n",
    "tester(X_train, y_train, net_ft, 128, 'ft', vocab_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task1: For Test set using Fast Text\n",
      "Loss is 7.818 and Accuracy for window size 5 is 3.463 \n"
     ]
    }
   ],
   "source": [
    "print(\"Task1: For Test set using Fast Text\")\n",
    "tester(X_test, y_test, net_ft, 128, 'ft', vocab_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 ( One Hot )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_net = Network(256, len(vocab), len(vocab), 1, timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('param_epoch_net_new_30.pkl', 'rb') as f:\n",
    "    tada = pickle.load(f)\n",
    "half_net.load_param(tada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_for_task2(filename, half_net, window = 5):\n",
    "    file = open(filename, 'r')\n",
    "    accuracy = 0\n",
    "    counter = 0\n",
    "    for line in file:\n",
    "        counter += 1\n",
    "        s = clean_document(line)\n",
    "        s.append('<EOS>')\n",
    "        inp_len = int(len(s)/2)\n",
    "        acc = 0\n",
    "        X,y = one_hot_creator([s[0:inp_len]],[], input_len = inp_len)\n",
    "        o = half_net.forward(X,ts = inp_len)\n",
    "     #   print(X.shape)\n",
    "        t = np.argsort(o[0])[-window:]\n",
    "        l = []\n",
    "        for el in t:\n",
    "            l.append(reverse_vocab[el])\n",
    "        if s[inp_len] in l:\n",
    "            acc += 1\n",
    "        X_n = X[0]\n",
    "        for i in range(inp_len, len(s)-1):\n",
    "            half_net.gru.c_initial = half_net.gru.clist[-1]\n",
    "            half_net.gru.clist = []\n",
    "            k = np.zeros(len(vocab))\n",
    "            k[np.argmax(o[0])] = 1\n",
    "    #         print(np.array([k]).shape)\n",
    "            #X_n = np.concatenate((X_n[1:], np.reshape(k, (1,k.shape[0]))), axis = 0)\n",
    "            o = half_net.forward(np.array([[k]]),ts = 1)\n",
    "            t = np.argsort(o[0])[-window:]\n",
    "            l = []\n",
    "            for el in t:\n",
    "                l.append(reverse_vocab[el])\n",
    "            if s[i+1] in l:\n",
    "                acc += 1\n",
    "        \n",
    "        accuracy += acc/len(s)\n",
    "    return(100*accuracy/counter)\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Accuracy (One-Hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task2: For Train set using One Hot: \n",
      "0.14793478297633417\n"
     ]
    }
   ],
   "source": [
    "print(\"Task2: For Train set using One Hot: \")\n",
    "acc_for_task2('train.txt', half_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task2: For Test set using One Hot: \n",
      "0.3300762497842789\n"
     ]
    }
   ],
   "source": [
    "print(\"Task2: For Test set using One Hot: \")\n",
    "acc_for_task2('test.txt', half_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 ( Fast Text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_net_ft = Network(256, 100, len(vocab), 1, timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('param_epoch_net_ft_65.pkl', 'rb') as f:\n",
    "    tada = pickle.load(f)\n",
    "half_net_ft.load_param(tada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_for_task2_ft(filename, half_net, window = 5):\n",
    "    file = open(filename, 'r')\n",
    "    accuracy = 0\n",
    "    counter = 0\n",
    "    for line in file:\n",
    "        counter += 1\n",
    "        s = clean_document(line)\n",
    "        s.append('<EOS>')\n",
    "        inp_len = int(len(s)/2)\n",
    "        acc = 0\n",
    "        X,y = ft_embed([s[0:inp_len]],[], vocab=vocab_ft)\n",
    "        o = half_net.forward(X,ts = inp_len)\n",
    "     #   print(X.shape)\n",
    "        t = np.argsort(o[0])[-window:]\n",
    "        l = []\n",
    "        for el in t:\n",
    "            l.append(reverse_vocab[el])\n",
    "        if s[inp_len] in l:\n",
    "            acc += 1\n",
    "        \n",
    "        for i in range(inp_len, len(s)-1):\n",
    "            half_net.gru.c_initial = half_net.gru.clist[-1]\n",
    "            half_net.gru.clist = []\n",
    "#             k = np.zeros(len(vocab))\n",
    "#             k[np.argmax(o[0])] = 1\n",
    "            k = vocab_ft[reverse_vocab[np.argmax(o[0])]]\n",
    "    #         print(np.array([k]).shape)\n",
    "            #X_n = np.concatenate((X_n[1:], np.reshape(k, (1,k.shape[0]))), axis = 0)\n",
    "            o = half_net.forward(np.array([[k]]),ts = 1)\n",
    "            t = np.argsort(o[0])[-window:]\n",
    "            l = []\n",
    "            for el in t:\n",
    "                l.append(reverse_vocab[el])\n",
    "            if s[i+1] in l:\n",
    "                acc += 1\n",
    "        \n",
    "        accuracy += acc/len(s)\n",
    "    return(100*accuracy/counter)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Accuracy (Fast Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task2: For Train set using Fast text: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.027032293652792267"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Task2: For Train set using Fast text: \")\n",
    "acc_for_task2_ft('train.txt', half_net_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task2: For Test set using Fast text: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.013271400132714002"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Task2: For Test set using Fast text: \")\n",
    "acc_for_task2_ft('test.txt', half_net_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fpenv",
   "language": "python",
   "name": "fpenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
