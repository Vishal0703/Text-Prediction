{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_document(doco):\n",
    "    punctuation = string.punctuation + '\\n\\n'\n",
    "    punc_replace = ''.join([' ' for s in punctuation])\n",
    "    doco_clean = doco.replace('-', ' ')\n",
    "    doco_alphas = re.sub(r'\\W +', '', doco_clean)\n",
    "    trans_table = str.maketrans(punctuation, punc_replace)\n",
    "    doco_clean = ' '.join([word.translate(trans_table) for word in doco_alphas.split(' ')])\n",
    "    doco_clean = doco_clean.split(' ')\n",
    "    doco_clean = [word.lower() for word in doco_clean if len(word) > 0]\n",
    "    \n",
    "    return doco_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_creator(file, inp_len):\n",
    "    X = []\n",
    "    label = []\n",
    "    for line in file:\n",
    "        cline = clean_document(line)\n",
    "        length = len(cline)\n",
    "        if length <= inp_len:\n",
    "            continue\n",
    "        for i in range(0, length - inp_len):\n",
    "            X.append(cline[i:i+inp_len])\n",
    "            label.append(cline[i+inp_len])\n",
    "        X.append(cline[i+1:])\n",
    "        label.append('<EOS>')\n",
    "    return X, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_train = open('train.txt')\n",
    "file_test = open('test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, y_train = dataset_creator(file_train, timesteps)\n",
    "X_test, y_test = dataset_creator(file_test, timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "i = 0\n",
    "for row in X_train:\n",
    "    for el in row:\n",
    "        if el not in vocab:\n",
    "            vocab[el] = i\n",
    "            i += 1\n",
    "            \n",
    "for row in X_test:\n",
    "    for el in row:\n",
    "        if el not in vocab:\n",
    "            vocab[el] = i\n",
    "            i += 1\n",
    "vocab['<EOS>'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_vocab = {}\n",
    "for key in vocab.keys():\n",
    "    reverse_vocab[vocab[key]] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_vocab[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_creator(X,y, vocab = vocab, input_len = timesteps):\n",
    "    vocab_len = len(vocab)\n",
    "    one_hotX = []\n",
    "    one_hotY = []\n",
    "    for row in X:\n",
    "        temp = np.zeros(shape = (input_len, vocab_len))\n",
    "        for (i,el) in enumerate(row):\n",
    "            temp[i][vocab[el]] = 1\n",
    "        one_hotX.append(temp)\n",
    "            \n",
    "    for row in y:\n",
    "        temp = np.zeros(shape = (vocab_len,))\n",
    "        temp[vocab[row]] = 1\n",
    "        one_hotY.append(temp)\n",
    "        \n",
    "    return np.array(one_hotX), np.array(one_hotY)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_oh, y_train_oh = one_hot_creator(X_train[:128], y_train[:128])   #one_hot vectors\n",
    "# # X_test_oh, y_test_oh = one_hot_creator(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = 0\n",
    "for i in y_train_oh[0]:\n",
    "    if i == 1:\n",
    "        z += 1\n",
    "        \n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'third', 'was']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 6683)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.ones((3,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = np.ones((1000,5))\n",
    "f = np.ones(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1001., 1001., 1001., 1001., 1001.],\n",
       "       [1001., 1001., 1001., 1001., 1001.],\n",
       "       [1001., 1001., 1001., 1001., 1001.]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = np.dot(d,e) + f\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.31590559, 1.21920605],\n",
       "       [2.31590559, 1.21920605],\n",
       "       [2.31590559, 1.21920605]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.ones((3,2))\n",
    "b = np.ones((3,2))\n",
    "c = np.random.uniform((3,2))\n",
    "\n",
    "z = np.multiply(a,b)\n",
    "np.multiply(z,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU():\n",
    "    def __init__(self,hidden_units, embed_len, batch_size, timesteps):\n",
    "        \n",
    "        self.hidden_units = hidden_units\n",
    "        self.Wc = np.random.normal(size = (embed_len + self.hidden_units, self.hidden_units)) \n",
    "        self.Wu = np.random.normal(size = (embed_len + self.hidden_units, self.hidden_units)) \n",
    "        self.bc = np.random.normal(size = (1, self.hidden_units)) \n",
    "        self.bu = np.random.normal(size = (1, self.hidden_units))         \n",
    "        self.batch_size = batch_size\n",
    "        self.clist = []\n",
    "        self.glist = []\n",
    "        self.tlist = []\n",
    "        self.c_initial = np.random.normal(size = (self.batch_size, self.hidden_units))\n",
    "        self.timesteps = timesteps\n",
    "        \n",
    "    def forward(self,X):\n",
    "        c = self.c_initial\n",
    "        for i in range(self.timesteps):\n",
    "            conc_inp = np.concatenate((X[:,i,:], c), axis = 1)\n",
    "            \n",
    "            tilda_inp = np.dot(conc_inp, self.Wc) + self.bc\n",
    "            c_tilda = self.tanh(tilda_inp)\n",
    "            self.tlist.append(tilda_inp)\n",
    "            \n",
    "            gamma_inp = np.dot(conc_inp, self.Wu) + self.bu\n",
    "            gammau = self.sigmoid(gamma_inp)\n",
    "            self.glist.append(gamma_inp)\n",
    "            \n",
    "            c = np.multiply(gammau, c_tilda) + np.multiply(1-gammau, c)\n",
    "            self.clist.append(c)\n",
    "        \n",
    "        return c  \n",
    "    \n",
    "    def backward(self, prev_dev, X, lr, gmin, gmax):\n",
    "        \n",
    "        for i in range(self.timesteps):\n",
    "            ind = self.timesteps - 1 - i\n",
    "            if ind > 0:\n",
    "                conc_inp = np.concatenate((X[:,ind,:], self.clist[ind-1]), axis = 1)\n",
    "            else:\n",
    "                conc_inp = np.concatenate((X[:,ind,:], self.c_initial), axis = 1)\n",
    "            \n",
    "            gammau = self.sigmoid(self.glist[ind])\n",
    "            \n",
    "            tanderv = 1 - (self.tanh(self.tlist[ind]))**2\n",
    "            sigmaderv = gammau*(1-gammau)\n",
    "            \n",
    "            c_tilda = self.tanh(self.tlist[ind])\n",
    "            inp_transpose = np.transpose(conc_inp)\n",
    "            \n",
    "            temp = np.multiply(gammau, tanderv)\n",
    "            temp = np.multiply(temp, prev_dev)\n",
    "            \n",
    "            gradWc = np.clip(np.dot(inp_transpose, temp)/self.batch_size, gmin, gmax)\n",
    "            gradbc = np.clip(np.sum(temp, axis = 0)/self.batch_size, gmin, gmax)\n",
    "            \n",
    "            if ind > 0:\n",
    "                temp = np.multiply(c_tilda - self.clist[ind - 1], sigmaderv)\n",
    "            else:\n",
    "                temp = np.multiply(c_tilda - self.c_initial, sigmaderv)\n",
    "            \n",
    "            temp = np.multiply(temp, prev_dev) \n",
    "            \n",
    "            gradWu = np.clip(np.dot(inp_transpose, temp)/self.batch_size, gmin, gmax)\n",
    "            gradbu = np.clip(np.sum(temp, axis = 0)/self.batch_size, gmin, gmax)\n",
    "            \n",
    "            self.Wc -= lr*gradWc\n",
    "            self.bc -= lr*gradbc\n",
    "            self.Wu -= lr*gradWu\n",
    "            self.bu -= lr*gradbu\n",
    "        \n",
    "        self.clist = []\n",
    "        self.tlist = []\n",
    "        self.glist = []\n",
    "        \n",
    "    def sigmoid(self, X):\n",
    "        return ( 1/ (1 + np.exp(-X)))\n",
    "    \n",
    "    def tanh(self, X):\n",
    "        p = np.exp(X)\n",
    "        m = np.exp(-X)\n",
    "        return((p-m)/(p+m))  \n",
    "    \n",
    "    def load_param(self, param):\n",
    "        self.Wc = param['Wc']\n",
    "        self.bc = param['bc']\n",
    "        self.Wu = param['Wu']\n",
    "        self.bu = param['bu']\n",
    "        \n",
    "    def save_param(self, param):\n",
    "        param['Wc'] = self.Wc\n",
    "        param['bc'] = self.bc\n",
    "        param['Wu'] = self.Wu\n",
    "        param['bu'] = self.bu\n",
    "        return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network():\n",
    "    def __init__(self, hidden_units, embed_len, output_size, batch_size, timesteps):\n",
    "        self.gru = GRU(hidden_units, embed_len, batch_size, timesteps)\n",
    "        self.W = np.random.normal(size = (hidden_units, output_size))\n",
    "        self.b = np.random.normal(size = (1, output_size))\n",
    "        self.batch_size = batch_size\n",
    "        self.c = 0\n",
    "        self.o = 0\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.c = self.gru.forward(X)\n",
    "        self.o = self.softmax(np.dot(self.c, self.W) + self.b)\n",
    "        return self.o\n",
    "    \n",
    "    def backward(self, X, y, lr, gmin, gmax):\n",
    "        grad = self.o - y\n",
    "\n",
    "        gradW = np.clip(np.dot(np.transpose(self.c), grad)/self.batch_size, gmin, gmax)\n",
    "        gradb = np.clip(np.sum(grad, axis = 0)/self.batch_size, gmin, gmax)\n",
    "\n",
    "        self.c = 0\n",
    "        self.o = 0\n",
    "                        \n",
    "        grad_to_backprop = np.dot(grad, np.transpose(self.W))\n",
    "        \n",
    "        self.W -= lr*gradW\n",
    "        self.b -= lr*gradb\n",
    "        \n",
    "        self.gru.backward(grad_to_backprop, X, lr, gmin, gmax)\n",
    "        \n",
    "    def softmax(self, X):\n",
    "        exps = np.exp(X - np.reshape(np.max(X, axis = 1), (X.shape[0], 1)))\n",
    "        return exps / np.reshape(np.sum(exps, axis = 1), (X.shape[0], 1))\n",
    "    \n",
    "    def load_param(self, param):\n",
    "        self.W = param['W']\n",
    "        self.b = param['b']\n",
    "        self.gru.load_param(param)\n",
    "        \n",
    "    def save_param(self):\n",
    "        param = {}\n",
    "        param['W'] = self.W\n",
    "        param['b'] = self.b\n",
    "        return(self.gru.save_param(param))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_calc(pred, actual):\n",
    "    mult = np.multiply(np.log(pred), actual)\n",
    "    return -np.sum(mult)/pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, net, epochs, lr, loss_list, batch_size, gmin = -10, gmax = 10, time_to_save = 1, cont_from = 0):\n",
    "    train_len = len(X)\n",
    "    for i in range(1, epochs+1):\n",
    "        tic = time.time()\n",
    "        counter = 0\n",
    "        \n",
    "        for j in range(0, train_len, batch_size):\n",
    "            \n",
    "            if(j+batch_size > train_len):\n",
    "                X_batch, y_batch = one_hot_creator(X[train_len-batch_size:], y[train_len-batch_size:])\n",
    "            else:\n",
    "                X_batch, y_batch = one_hot_creator(X[j:j+batch_size], y[j:j+batch_size])\n",
    "                \n",
    "            pred = net.forward(X_batch)\n",
    "            loss = loss_calc(pred, y_batch)\n",
    "            \n",
    "            loss_list.append(loss)\n",
    "            net.backward(X_batch, y_batch, lr, gmin, gmax)\n",
    "            counter += 1\n",
    "            \n",
    "        if i % time_to_save == 0:\n",
    "            param_dict = net.save_param()\n",
    "            with open('param_epoch_' + str(i) + '.pkl', 'wb') as f:\n",
    "                pickle.dump(param_dict, f)\n",
    "        \n",
    "        ep_time = time.time() - tic\n",
    "        print(\"Epoch: %d --> Average Loss: %.3f completed in %.3f seconds\" \n",
    "              %(cont_from + i, sum(loss_list[len(loss_list)-counter:]) / counter, ep_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = Network(1024, len(vocab), len(vocab), 256, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('param_epoch_44.pkl', 'rb') as f:\n",
    "    tada = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2.load_param(tada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = one_hot_creator(X_train[:256], y_train[:256])\n",
    "p = net2.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 6683)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted artists actual being\n",
      "predicted background actual run\n",
      "predicted prisoner actual by\n",
      "predicted quickly actual the\n",
      "predicted kuwaiti actual head\n",
      "predicted lighter actual of\n",
      "predicted anyways actual an\n",
      "predicted covering actual investment\n",
      "predicted furnishings actual firm\n",
      "predicted kenneally actual <EOS>\n",
      "predicted nov15 actual he\n",
      "predicted pay actual was\n",
      "predicted kosher actual manipulating\n",
      "predicted belt actual the\n",
      "predicted rate actual market\n",
      "predicted an actual with\n",
      "predicted successors actual his\n",
      "predicted flowers actual bombing\n",
      "predicted jeez actual targets\n",
      "predicted insist actual <EOS>\n",
      "predicted carless actual engineer\n",
      "predicted someday actual asi\n",
      "predicted vsmiami actual ali\n",
      "predicted gold actual from\n",
      "predicted initiated actual tikrit\n",
      "predicted control actual <EOS>\n",
      "predicted mumbai actual also\n",
      "predicted gently actual killed\n",
      "predicted ham actual in\n",
      "predicted drill actual the\n",
      "predicted feeding actual attack\n",
      "predicted hairdresser actual <EOS>\n",
      "predicted blogosphere actual it\n",
      "predicted direct actual had\n",
      "predicted neighbourhood actual no\n",
      "predicted mcgilloway actual legal\n",
      "predicted alternative actual grounds\n",
      "predicted write actual for\n",
      "predicted entirely actual such\n",
      "predicted stranger actual an\n",
      "predicted senior actual exclusion\n",
      "predicted msdoyon actual <EOS>\n",
      "predicted bare actual a\n",
      "predicted creativity actual small\n",
      "predicted cars actual one\n",
      "predicted hunger actual and\n",
      "predicted alright actual easily\n",
      "predicted requirements actual missed\n",
      "predicted global actual <EOS>\n",
      "predicted morton actual view\n",
      "predicted cockatiels actual it\n",
      "predicted raw actual is\n",
      "predicted 2day actual highly\n",
      "predicted teresa actual significant\n",
      "predicted heavily actual <EOS>\n",
      "predicted france actual iraq\n",
      "predicted committee actual is\n",
      "predicted adaptation actual only\n",
      "predicted trainer actual going\n",
      "predicted <EOS> actual to\n",
      "predicted 567 actual get\n",
      "predicted maps actual better\n",
      "predicted 40amount actual this\n",
      "predicted stylist actual way\n",
      "predicted bull actual <EOS>\n",
      "predicted woodland actual a\n",
      "predicted responds actual crime\n",
      "predicted gm actual against\n",
      "predicted wow actual humanity\n",
      "predicted forever actual prosecute\n",
      "predicted rental actual the\n",
      "predicted explosives actual person\n",
      "predicted jean actual <EOS>\n",
      "predicted distributed actual the\n",
      "predicted fiftythree actual court\n",
      "predicted cause actual managed\n",
      "predicted cruise actual to\n",
      "predicted consideration actual take\n",
      "predicted child actual his\n",
      "predicted enjoyed actual deposition\n",
      "predicted greetings actual before\n",
      "predicted temperature actual he\n",
      "predicted sending actual died\n",
      "predicted steers actual <EOS>\n",
      "predicted vague actual again\n",
      "predicted ecosystems actual nov28\n",
      "predicted purchase actual <EOS>\n",
      "predicted clh actual talk\n",
      "predicted determiner actual that\n",
      "predicted dodgy actual the\n",
      "predicted christmas actual night\n",
      "predicted retake actual curfew\n",
      "predicted kale actual might\n",
      "predicted gesture actual be\n",
      "predicted ifa actual implemented\n",
      "predicted satisfaction actual again\n",
      "predicted afflictions actual <EOS>\n",
      "predicted acting actual are\n",
      "predicted el actual reported\n",
      "predicted racing actual dead\n",
      "predicted run actual and\n",
      "predicted whom actual 500\n",
      "predicted atmosphere actual reported\n",
      "predicted baby actual wounded\n",
      "predicted boat actual in\n",
      "predicted crackdown actual fallujah\n",
      "predicted two actual alone\n",
      "predicted vsnew actual <EOS>\n",
      "predicted commercial actual that\n",
      "predicted racked actual icdc\n",
      "predicted status actual were\n",
      "predicted outstanding actual controlling\n",
      "predicted uon actual ramadi\n",
      "predicted 5250 actual <EOS>\n",
      "predicted responses actual tour\n",
      "predicted billion actual chernobyl\n",
      "predicted cafes actual and\n",
      "predicted nonjudgmental actual write\n",
      "predicted 483 actual your\n",
      "predicted rid actual own\n",
      "predicted bending actual story\n",
      "predicted ineos actual <EOS>\n",
      "predicted christmas actual a\n",
      "predicted professional actual post\n",
      "predicted native actual about\n",
      "predicted 3393 actual faultfinding\n",
      "predicted entering actual or\n",
      "predicted romethanks actual assigning\n",
      "predicted cloud actual blame\n",
      "predicted unbeatable actual <EOS>\n",
      "predicted effective actual lot\n",
      "predicted python actual to\n",
      "predicted ants actual learn\n",
      "predicted players actual about\n",
      "predicted private actual chernobyl\n",
      "predicted models actual <EOS>\n",
      "predicted countries actual wealth\n",
      "predicted globe actual of\n",
      "predicted noc actual references\n",
      "predicted jana actual on\n",
      "predicted smoke actual chernobyl\n",
      "predicted 1584 actual <EOS>\n",
      "predicted verified actual the\n",
      "predicted article actual following\n",
      "predicted radiation actual links\n",
      "predicted gouging actual and\n",
      "predicted practice actual draw\n",
      "predicted cedar actual your\n",
      "predicted talent actual own\n",
      "predicted performance actual conclusions\n",
      "predicted breakfast actual <EOS>\n",
      "predicted mrslynda actual shoe\n",
      "predicted facts actual sizing\n",
      "predicted software actual machines\n",
      "predicted summary actual that\n",
      "predicted conclusion actual was\n",
      "predicted certified actual a\n",
      "predicted fire actual form\n",
      "predicted officiol actual of\n",
      "predicted usage actual radiation\n",
      "predicted bike actual xray\n",
      "predicted petshoppe actual <EOS>\n",
      "predicted ebs actual friend\n",
      "predicted edge actual <EOS>\n",
      "predicted dial actual a\n",
      "predicted faith actual way\n",
      "predicted afterward actual to\n",
      "predicted slightly actual signal\n",
      "predicted keying actual priorities\n",
      "predicted fruit actual <EOS>\n",
      "predicted <EOS> actual to\n",
      "predicted arbitration actual be\n",
      "predicted attempts actual defanged\n",
      "predicted practicing actual <EOS>\n",
      "predicted fan actual fdr\n",
      "predicted accident actual in\n",
      "predicted covering actual iraq\n",
      "predicted kaoshikii actual not\n",
      "predicted thx actual a\n",
      "predicted outstanding actual ronald\n",
      "predicted warned actual reagan\n",
      "predicted thematically actual <EOS>\n",
      "predicted particular actual remember\n",
      "predicted purchase actual a\n",
      "predicted genfoulkesstraat actual seminal\n",
      "predicted blooming actual bush\n",
      "predicted hard actual moment\n",
      "predicted frisco actual in\n",
      "predicted skin actual 1999\n",
      "predicted flowing actual <EOS>\n",
      "predicted thumpstar actual s\n",
      "predicted touch actual pop\n",
      "predicted nearby actual quiz\n",
      "predicted harrison actual on\n",
      "predicted error actual international\n",
      "predicted natasha actual <EOS>\n",
      "predicted gtownsend actual 3\n",
      "predicted westfield actual 29\n",
      "predicted walmart actual p\n",
      "predicted never actual mest\n",
      "predicted clh actual 2029\n",
      "predicted webpage actual gmt\n",
      "predicted driveway actual <EOS>\n",
      "predicted rejuvenate actual name\n",
      "predicted achieving actual the\n",
      "predicted sitara actual others\n",
      "predicted rehearing actual <EOS>\n",
      "predicted clare actual the\n",
      "predicted heating actual general\n",
      "predicted reformer actual who\n",
      "predicted limbs actual is\n",
      "predicted busy actual in\n",
      "predicted eaten actual charge\n",
      "predicted inspection actual of\n",
      "predicted kosher actual pakistan\n",
      "predicted which actual <EOS>\n",
      "predicted hunger actual this\n",
      "predicted origin actual 50\n",
      "predicted schedulers actual questions\n",
      "predicted device actual asked\n",
      "predicted jean actual bush\n",
      "predicted wdon actual <EOS>\n",
      "predicted exceed actual president\n",
      "predicted deserve actual clinton\n",
      "predicted gang actual also\n",
      "predicted holding actual criticized\n",
      "predicted courtney actual bush\n",
      "predicted pst actual s\n",
      "predicted gtee actual comments\n",
      "predicted neat actual <EOS>\n",
      "predicted marianne actual t\n",
      "predicted willingly actual see\n",
      "predicted 2sided actual bush\n",
      "predicted visualisations actual doing\n",
      "predicted action actual any\n",
      "predicted shown actual of\n",
      "predicted enclosed actual this\n",
      "predicted bush actual <EOS>\n",
      "predicted associate actual actually\n",
      "predicted kendel actual weakened\n",
      "predicted bugless actual israeli\n",
      "predicted gastroenteritis actual security\n",
      "predicted mechanism actual <EOS>\n",
      "predicted uvb actual anything\n",
      "predicted aiding actual practical\n",
      "predicted nasrallah actual to\n",
      "predicted strippers actual help\n",
      "predicted manipulating actual the\n",
      "predicted romantic actual palestinians\n",
      "predicted townhouse actual <EOS>\n",
      "predicted excluding actual allow\n",
      "predicted mhuber actual palestinian\n",
      "predicted harmony actual radicals\n",
      "predicted contribution actual to\n",
      "predicted felicia actual launch\n",
      "predicted ammount actual operations\n"
     ]
    }
   ],
   "source": [
    "yint = np.argmax(y, axis = 1)\n",
    "pint = np.argmax(p, axis = 1)\n",
    "for i in range(pint.shape[0]):\n",
    "    print(\"predicted \" + str(reverse_vocab[pint[i]]) + \" actual \" + str(reverse_vocab[yint[i]]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yint[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114.23882330678707"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summer = 0\n",
    "for i in range(256):\n",
    "    summer += np.log(p[i][yint[i]])\n",
    "    \n",
    "-summer/256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 --> Average Loss: 56.538 completed in 110.000 seconds\n",
      "Epoch: 2 --> Average Loss: 54.516 completed in 110.083 seconds\n",
      "Epoch: 3 --> Average Loss: 53.196 completed in 110.757 seconds\n",
      "Epoch: 4 --> Average Loss: 52.092 completed in 111.730 seconds\n",
      "Epoch: 5 --> Average Loss: 51.070 completed in 108.668 seconds\n",
      "Epoch: 6 --> Average Loss: 50.199 completed in 110.120 seconds\n",
      "Epoch: 7 --> Average Loss: 49.365 completed in 111.043 seconds\n",
      "Epoch: 8 --> Average Loss: 48.562 completed in 110.859 seconds\n",
      "Epoch: 9 --> Average Loss: 47.839 completed in 111.537 seconds\n",
      "Epoch: 10 --> Average Loss: 47.220 completed in 108.511 seconds\n",
      "Epoch: 11 --> Average Loss: 46.601 completed in 110.023 seconds\n",
      "Epoch: 12 --> Average Loss: 45.974 completed in 111.316 seconds\n",
      "Epoch: 13 --> Average Loss: 45.424 completed in 109.929 seconds\n",
      "Epoch: 14 --> Average Loss: 44.875 completed in 115.560 seconds\n",
      "Epoch: 15 --> Average Loss: 44.303 completed in 111.134 seconds\n",
      "Epoch: 16 --> Average Loss: 43.813 completed in 110.129 seconds\n",
      "Epoch: 17 --> Average Loss: 43.341 completed in 109.214 seconds\n",
      "Epoch: 18 --> Average Loss: 42.902 completed in 110.003 seconds\n",
      "Epoch: 19 --> Average Loss: 42.526 completed in 111.369 seconds\n",
      "Epoch: 20 --> Average Loss: 42.146 completed in 111.001 seconds\n",
      "Epoch: 21 --> Average Loss: 41.765 completed in 111.149 seconds\n",
      "Epoch: 22 --> Average Loss: 41.424 completed in 110.096 seconds\n",
      "Epoch: 23 --> Average Loss: 41.111 completed in 110.398 seconds\n",
      "Epoch: 24 --> Average Loss: 40.843 completed in 110.522 seconds\n",
      "Epoch: 25 --> Average Loss: 40.572 completed in 109.608 seconds\n",
      "Epoch: 26 --> Average Loss: 40.287 completed in 109.904 seconds\n",
      "Epoch: 27 --> Average Loss: 40.028 completed in 112.674 seconds\n",
      "Epoch: 28 --> Average Loss: 39.790 completed in 112.054 seconds\n",
      "Epoch: 29 --> Average Loss: 39.559 completed in 118.679 seconds\n",
      "Epoch: 30 --> Average Loss: 39.324 completed in 110.005 seconds\n",
      "Epoch: 31 --> Average Loss: 39.113 completed in 110.754 seconds\n",
      "Epoch: 32 --> Average Loss: 38.891 completed in 111.651 seconds\n",
      "Epoch: 33 --> Average Loss: 38.671 completed in 109.227 seconds\n",
      "Epoch: 34 --> Average Loss: 38.480 completed in 112.102 seconds\n",
      "Epoch: 35 --> Average Loss: 38.265 completed in 111.180 seconds\n",
      "Epoch: 36 --> Average Loss: 38.051 completed in 111.897 seconds\n",
      "Epoch: 37 --> Average Loss: 37.846 completed in 109.779 seconds\n",
      "Epoch: 38 --> Average Loss: 37.641 completed in 111.003 seconds\n",
      "Epoch: 39 --> Average Loss: 37.409 completed in 111.447 seconds\n",
      "Epoch: 40 --> Average Loss: 37.194 completed in 111.449 seconds\n",
      "Epoch: 41 --> Average Loss: 37.001 completed in 111.684 seconds\n",
      "Epoch: 42 --> Average Loss: 36.798 completed in 111.593 seconds\n",
      "Epoch: 43 --> Average Loss: 36.612 completed in 111.496 seconds\n",
      "Epoch: 44 --> Average Loss: 36.451 completed in 110.967 seconds\n",
      "Epoch: 45 --> Average Loss: 36.312 completed in 111.822 seconds\n",
      "Epoch: 46 --> Average Loss: 36.174 completed in 105.519 seconds\n",
      "Epoch: 47 --> Average Loss: 36.011 completed in 99.454 seconds\n",
      "Epoch: 48 --> Average Loss: 35.820 completed in 99.415 seconds\n",
      "Epoch: 49 --> Average Loss: 35.618 completed in 100.442 seconds\n",
      "Epoch: 50 --> Average Loss: 35.438 completed in 100.310 seconds\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "train(X_train, y_train, net, 50, 0.01, loss_list, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51 --> Average Loss: 35.246 completed in 104.009 seconds\n",
      "Epoch: 52 --> Average Loss: 35.230 completed in 105.677 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-620dbd1d1fe8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcont_from\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-110-860fbed2d9fa>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(X, y, net, epochs, lr, loss_list, batch_size, gmin, gmax, time_to_save, cont_from)\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_calc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-f643dd352f90>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-f643dd352f90>\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mexps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mexps\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(X_train, y_train, net, 50, 0.001, loss_list, 128, cont_from=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 --> Average Loss: 114.529 completed in 280.236 seconds\n",
      "Epoch: 2 --> Average Loss: 111.436 completed in 278.928 seconds\n",
      "Epoch: 3 --> Average Loss: 108.830 completed in 279.059 seconds\n",
      "Epoch: 4 --> Average Loss: 106.986 completed in 278.852 seconds\n",
      "Epoch: 5 --> Average Loss: 105.232 completed in 280.288 seconds\n",
      "Epoch: 6 --> Average Loss: 103.848 completed in 278.417 seconds\n",
      "Epoch: 7 --> Average Loss: 102.589 completed in 279.409 seconds\n",
      "Epoch: 8 --> Average Loss: 101.249 completed in 279.429 seconds\n",
      "Epoch: 9 --> Average Loss: 100.077 completed in 277.888 seconds\n",
      "Epoch: 10 --> Average Loss: 99.031 completed in 277.133 seconds\n",
      "Epoch: 11 --> Average Loss: 97.971 completed in 278.471 seconds\n",
      "Epoch: 12 --> Average Loss: 96.830 completed in 256.688 seconds\n",
      "Epoch: 13 --> Average Loss: 95.764 completed in 249.403 seconds\n",
      "Epoch: 14 --> Average Loss: 94.895 completed in 250.579 seconds\n",
      "Epoch: 15 --> Average Loss: 94.046 completed in 252.558 seconds\n",
      "Epoch: 16 --> Average Loss: 93.177 completed in 248.827 seconds\n",
      "Epoch: 17 --> Average Loss: 92.318 completed in 249.751 seconds\n",
      "Epoch: 18 --> Average Loss: 91.394 completed in 250.058 seconds\n",
      "Epoch: 19 --> Average Loss: 90.630 completed in 249.594 seconds\n",
      "Epoch: 20 --> Average Loss: 89.892 completed in 249.921 seconds\n",
      "Epoch: 21 --> Average Loss: 89.278 completed in 249.828 seconds\n",
      "Epoch: 22 --> Average Loss: 88.601 completed in 249.257 seconds\n",
      "Epoch: 23 --> Average Loss: 87.937 completed in 249.519 seconds\n",
      "Epoch: 24 --> Average Loss: 87.303 completed in 250.289 seconds\n",
      "Epoch: 25 --> Average Loss: 86.834 completed in 248.661 seconds\n",
      "Epoch: 26 --> Average Loss: 86.304 completed in 250.428 seconds\n",
      "Epoch: 27 --> Average Loss: 85.712 completed in 249.247 seconds\n",
      "Epoch: 28 --> Average Loss: 85.087 completed in 251.220 seconds\n",
      "Epoch: 29 --> Average Loss: 84.558 completed in 249.313 seconds\n",
      "Epoch: 30 --> Average Loss: 84.042 completed in 256.304 seconds\n",
      "Epoch: 31 --> Average Loss: 83.512 completed in 248.796 seconds\n",
      "Epoch: 32 --> Average Loss: 83.014 completed in 250.137 seconds\n",
      "Epoch: 33 --> Average Loss: 82.599 completed in 250.029 seconds\n",
      "Epoch: 34 --> Average Loss: 82.164 completed in 249.020 seconds\n",
      "Epoch: 35 --> Average Loss: 81.574 completed in 248.932 seconds\n",
      "Epoch: 36 --> Average Loss: 81.043 completed in 250.424 seconds\n",
      "Epoch: 37 --> Average Loss: 80.600 completed in 248.351 seconds\n",
      "Epoch: 38 --> Average Loss: 80.312 completed in 269.794 seconds\n"
     ]
    }
   ],
   "source": [
    "loss_list2 = []\n",
    "train(X_train, y_train, net2, 50, 0.01, loss_list2, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.918617963790894"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toc - tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 --> Average Loss: 0.250\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "j = 0.25\n",
    "print(\"Epoch: %d --> Average Loss: %.3f\" %(i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in range(1,10):\n",
    "    l.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 5\n",
    "with open('param_' + str(j) + 'pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('param' + j + '.pkl', 'rb') as f:\n",
    "    tada = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6683"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildmodel(VOC_LEN, inp_len = input_len):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, input_shape = (inp_len, VOC_LEN), return_sequences = True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(VOC_LEN, activation = 'softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = buildmodel(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_3 to have 2 dimensions, but got array with shape (1000, 6683, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-4a2622fcfdb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_oh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_oh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Sem8/flipkart/fpenv/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Sem8/flipkart/fpenv/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Sem8/flipkart/fpenv/lib/python3.5/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_3 to have 2 dimensions, but got array with shape (1000, 6683, 1)"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_oh, y_train_oh, epochs = 50, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fpenv",
   "language": "python",
   "name": "fpenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
